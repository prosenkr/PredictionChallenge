---
title: "Prediction Challenge"
subtitle: "In Machine Learning and Artificial Intelligence"
author: "Justin Engelker, Philipp Rosenkranz, Fabian Günkel, Lennard Kothe, Paul Wieder"
date: "21.12.2022"
output:
  pdf_document:
    number_sections: yes
header-includes:
- \usepackage{graphicx}
- \usepackage{float}
- \usepackage{amsmath}
- \usepackage{dcolumn}
- \usepackage{parskip}
- \usepackage[backend=bibtex]{biblatex}
- \addbibresource{references.bib}
- \usepackage{xcolor}
---

```{r, echo=FALSE}
# Setup
rm(list=ls())
set.seed(1234)
options(scipen=10000)
select <- dplyr::select
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, message = FALSE, warning = FALSE)
# Load Libraries
library(tidyverse)
library(tidybayes)
library(funModeling) 
library(scico) 
library(ggrepel)
library(scico)
library(ggthemes)
library(patchwork)
library(haven)
library(tibble)
library(knitr)
library(tinytex)
library(sandwich)
library(lmtest)
library(stargazer)
library(tidymodels)
library(ISLR)
library(GGally)
library(broom)
library(dotwhisker)
library(performance)
library(funModeling)
library(sjPlot)
library(dplyr)
library(data.table)
library(stringi)
library(Hmisc)
library(skimr)
library(tm)
library(caret)
library(naivebayes)

options(digits = 3)
```

\newpage
\tableofcontents

\newpage
\setlength{\parindent}{0em}
\baselineskip=1.5\baselineskip
\section{Introduction}

Over the past decade, the labour market has become increasingly tight. Companies have strong difficulties finding matching and convincing professionals and spend a big part of their investments in employee marketing and active sourcing. 
The tension on the labour market is also noticeable on the yachting and luxury shipping branch. M Yachts, a charter platform to book yachting experiences is highly affected by the described development. In addition, the yachting industry is affected by a high turnover of personnel working as crew members on boats and has very demanding and exclusive customers. For this reason, it is even more difficult and important to find suitable employees for companies in the yachting industry such as M Yachts. This paper addresses this issue and aims to answer the following research question by selected sing machine learning methods:

\begin{quote}
\emph{RQ: How well does an applicant fit into a position at M Yachts?}
\end{quote}

The fit of an applicant shall be represented by a star rating between zero and five, where five is a perfect fit and zero represents the lowest matching. The star rating is currently determined manually by a recruiter, which poses a lot of manual work and challenges the company’s resources. The motivation of the paper is to learn the application process to reduce those costs on the one hand and provide an objective judgement to improve the quality of the yachting employees on the other hand. 
The article is written as a prediction challenge of the module Machine Learning at the University of Cologne, held by Prof. Dr. Markus Weinmann, in cooperation with the company M Yachts. Goals of the module are to apply the main concepts of machine learning and analyse results of them, competencies which are applied to a practical problem of M Yachts in this task. 

First, the paper deals with exploratory data analysis and describes the characteristics of the given data set. Second, different machine learning methods are presented to find the best prediction whether an applicant fits M Yachts or not. After introducing the most important findings, the paper will discuss both usability and validity of the results, as well as ethical applicability. In the end, the article gives an outlook to illuminate the extensibility of the data set and the potential of implementing other Machine Learning Tools to improve the application process.


\section{Dataset}

To ensure that the data used for training machine learning models is of high quality, accurate and properly formatted, we define variables of interest, apply data wrangling and perform an exploratory data analysis (EDA) on the prepared dataset. The dataset is provided by M Yachts and can be viewed in the included csv file ([MyCrew Data_221208.csv](./MyCrew Data_221208.csv)).

```{r, echo=FALSE, message=FALSE}
# Load raw dataset
mycrew <- read.csv("MyCrew Data_221208.csv", na.strings = c("", "NA","na", "n/a", "N/A"))

# Replace data that leads to Latex error
mycrew$City <- replace(mycrew$City, mycrew$City=="Ден-Хелдер","Den Helder")
```
```{r, echo=FALSE, message=FALSE, results=FALSE}
summary(mycrew)
```

\subsection{Variables of Interest}

To optimise the model's performance we filter variables that are not suitable as predictors. Therefore, we obtained an overview of the raw data by examining each variable and checking for correlations between the variables and the outcome \textit{StarRating}. This leads to the exclusion of 15 variables, including variables we do not consider to be causally related to the Star Rating (\textit{WorkerID, IsTempProfile, WorkerJobSearchStatus, ConnectionPipelineStatus, ProfileAddedAt, ProfileAddedBy, TalentPoolCount}), variables that contain mostly missing data (\textit{RecruiterMessage, WorkerMessage, TotalResponseRate, LastCommunicatedAt, LastCommunicatedBy}) and variables about the origin of the people that are too specific and will be represented by the variable \textit{Country} (\textit{State, City, Postcode}). This leaves us with the four character variables \textit{CurrentCompany}, \textit{CurrentRole}, \textit{Country} and \textit{TalentCommunitySource} and two integer variables \textit{ExperienceYears} and \textit{CvStrength} of our selected dataset (see Figure 1).

```{r, cache=TRUE, echo=FALSE, results=FALSE}
# Number of NA's in each column:
sort(colSums(is.na(mycrew)))
```

\subsection{Data Wrangling}

Data Wrangling is referred to as "the process of gathering, selecting, and transforming data to answer an analytical question" \cite{elderresearch}, which in our case results in a prepared dataframe [df] used for various machine learning models (see Figure 1). Specifically, we transform our raw dataset by mutating relevant variables and by converting categorial variables to factors and numeric variables to numeric values. With data enrichment we aim to expand the scope and context of our analysis and potentially improve the accuracy of the prediction.

\subsubsection{Transforming Given Variables}

\begin{figure}
  \includegraphics{DataWranglingFig.pdf}
  \caption{Data Wrangling Process}
\end{figure}

The original variable \textit{TalentCommunitySource} is transformed so that its attributes are unified into "LinkedIn","Indeed","Google","my-crew.com" and "Other" and saved as \textit{TalentSource}. The variables \textit{CurrentRole} and \textit{CurrentCompany} are matched with job, respectively industry categories by specifying a list of keywords for each category and filtering the attributes that contain those keywords. Therefore, we use ifelse statements including the grepl function.

To get an overview of different categories in both columns, we first performed the unsupervised model of k-means clustering by transforming the words into numeric values and finding clusters within those values. By plotting the "Within groups sum of squares" we identified k=11 for the clustering of \textit{CurrentRole} (see Figure 2). This way we found clusters such as "Chef", "Manager" or "Master". The same procedure was applied to \textit{CurrentCompany} with k=13 clusters. With those insights we classified the variables into the following groups (n refers to the number of observations in each group in our final data frame):

\begin{table}[h!]
\raggedright
\begin{tabular}{|c|c c c c c c c c|} 
 \hline
 Jobs: & Allrounder & Captain & Chef & Deckhand & Engineer & Steward & Unemployed & Other \\ [0.5ex] 
 \hline
 n: & 23 & 155 & 164 & 69 & 74 & 83 & 20 & 494 \\ [1ex]
 \hline
\end{tabular}
\begin{tabular}{|c|c c c c c c c|} 
 \hline
 Industries: & airline & defence & freelance & gastro & hotel & yachting & Other \\ [0.5ex] 
 \hline
 n: & 50 & 50 & 15 & 77 & 59 & 222 & 610 \\ [1ex]
 \hline
\end{tabular}
\caption{Attributes in \textit{Jobs} and \textit{Industry}}
\label{table:1}
\end{table}

```{r, echo=FALSE, results=FALSE}
# Mutate TalentSource, convert and select variables 
df1 <- mycrew %>%
  mutate( StarRating = ifelse(StarRating == "OneStarReview",1,
                       ifelse(StarRating == "TwoStarReview",2,
                       ifelse(StarRating == "ThreeStarReview", 3,
                       ifelse(StarRating == "FourStarReview", 4,
                       ifelse(StarRating == "FiveStarReview", 5,"NA"))))),
          TalentSource = 
                        ifelse(TalentCommunitySource == "LinkedIn", "LinkedIn",
                        ifelse(TalentCommunitySource == "Careers Website",
                               "Careers Website",
                        ifelse(TalentCommunitySource == "Indeed","Indeed",
                        ifelse((grepl("linked", TalentCommunitySource, 
                                      ignore.case = TRUE)) == TRUE, "LinkedIn",
                        ifelse((grepl("google", TalentCommunitySource, 
                                      ignore.case = TRUE)) == TRUE, "Google",
                        ifelse((grepl("crew", TalentCommunitySource,
                                     ignore.case = TRUE)) == TRUE, "my-crew.com", 
                        "Other")))))), # unify TalentCommunitySource variable
          TalentSource = as.factor(TalentSource),
          StarRating = as.factor(StarRating),
          CurrentCompany = as.factor(CurrentCompany),
          CurrentRole = as.factor(CurrentRole),
          Country = as.factor(Country)) %>%
  select(CurrentCompany, CurrentRole, StarRating, Country, TalentSource, ExperienceYears, CvStrength)
```

```{r, cache=TRUE, echo=FALSE, results=FALSE}
# function for WSSPlot (k-means clustering)
# function from: https://rdrr.io/github/dgarmat/dgfunctionpack/man/wssplot.html
wssplot <- function(df, nc = 15, seed = 1234, nstart = 1){
  wss <- (nrow(df) - 1) * sum(apply(df, 2, var))
  for (i in 2:nc){
    set.seed(seed)
    wss[i] <- sum(kmeans(df, centers = i, nstart = nstart)$withinss)}
  qplot(x = 1:nc, y = wss,  xlab="Number of Clusters",
        ylab="Within groups sum of squares") + geom_line() +
    theme(axis.title.y = element_text(size = rel(.8), angle = 90)) +
    theme(axis.title.x = element_text(size = rel(.8), angle = 00)) +
    theme(axis.text.x = element_text(size = rel(.8))) +
    theme(axis.text.y = element_text(size = rel(.8)))
}
```

```{r, cache=TRUE, echo=FALSE, results=FALSE}
# Jobs k-means clustering:

# Transforming words into values in a matrix
jobs <- as.vector(df1$CurrentRole)

dtm_j <- DocumentTermMatrix(Corpus(VectorSource(jobs)))
```

```{r, cache=TRUE, echo=FALSE, fig.align="center", fig.pos="h!", out.width="70%", fig.cap="WSS Plot for k-means Clustering"}
# Plotting the "Within groups sum of squares" by number of clusters
#wssplot(dtm_j, nc=25,seed=1234,nstart=1)
```

```{r, cache=TRUE, echo=FALSE, results=FALSE}
# Selecting k = 11 clusters to perform k-means clustering
set.seed(1234)
km <- kmeans(as.matrix(dtm_j),centers=11, algorithm="Hartigan-Wong")

df_jobs <- data.frame(job = jobs, cluster = km$cluster)

#df_jobs %>%
  #filter(cluster==11)

# After printing out each cluster, we manually assign categories to each cluster  
df_jobs$cluster[df_jobs$cluster == 1] <- "Manager"
df_jobs$cluster[df_jobs$cluster == 2] <- "Attendant"
df_jobs$cluster[df_jobs$cluster == 3] <- "Other"
df_jobs$cluster[df_jobs$cluster == 4] <- "Captain"
df_jobs$cluster[df_jobs$cluster == 5] <- "Crew"
df_jobs$cluster[df_jobs$cluster == 6] <- "Officer"
df_jobs$cluster[df_jobs$cluster == 7] <- "Engineer"
df_jobs$cluster[df_jobs$cluster == 8] <- "Assistant/Instructor/Manager"
df_jobs$cluster[df_jobs$cluster == 9] <- "Master"
df_jobs$cluster[df_jobs$cluster == 10] <- "Captain" 
df_jobs$cluster[df_jobs$cluster == 11] <- "Chef"

df_jobs %>%
group_by(cluster) %>%
summarise(count = n())
```

```{r, echo=FALSE, results=FALSE}
# Same procedure for Company clustering:

# Transforming words into values in a matrix
companies <- as.vector(df1$CurrentCompany)

dtm_c <- DocumentTermMatrix(Corpus(VectorSource(companies)))

# Plotting the "Within groups sum of squares" by number of clusters
#wssplot(dtm_c, nc=25,seed=1234,nstart=1)

# Selecting k = 13 clusters to perform k-means clustering
set.seed(1234)
km <- kmeans(as.matrix(dtm_c),centers=13, algorithm="Hartigan-Wong")

df_comp <- data.frame(company = companies, cluster = km$cluster)

#df_comp %>%
 # filter(cluster==13)

# After printing out each cluster, we manually assign categories to each cluster
df_comp$cluster[df_comp$cluster == 1] <- "Restaurant"
df_comp$cluster[df_comp$cluster == 2] <- "The xy"
df_comp$cluster[df_comp$cluster == 3] <- "shipping"
df_comp$cluster[df_comp$cluster == 4] <- "Yacht"
df_comp$cluster[df_comp$cluster == 5] <- "Resort/Spa/Hotel"
df_comp$cluster[df_comp$cluster == 6] <- "International"
df_comp$cluster[df_comp$cluster == 7] <- "Other"
df_comp$cluster[df_comp$cluster == 8] <- "M/Y"
df_comp$cluster[df_comp$cluster == 9] <- "Company"
df_comp$cluster[df_comp$cluster == 10] <- "Hotel"
df_comp$cluster[df_comp$cluster == 11] <- "Carnival"
df_comp$cluster[df_comp$cluster == 12] <- "LLC"
df_comp$cluster[df_comp$cluster == 13] <- "Ltd"

df_comp %>%
group_by(cluster) %>%
summarise(count = n())
```

```{r, cache=TRUE, echo=FALSE, results=FALSE}
# Match CurrentRole with job categories:

# - Allrounder (if more than one job category is specified)
# - Captain
# - Deckhand
# - Engineer
# - Chef
# - Steward

# Add senior and leader to further specify "high quality" roles

chef <- c("chef", "cook", "cocin", "kitch","cozin", "culinar", "cuisin", "capoc", "sous")
engineer <- c("neer", "tech", "engin", "ingegn", "engen", "ingen")
deckhand <- c("deck", "bosun", "marin", "seaman", "mozzo", "shipmate", "cadet")
unemployed <- c("unempl", "seeking", "jobless", "search", "none")
steward <- c("stew", "cabin", "crew", "waiter", "waitress")
captain <- c("ptain", "skipper", "capit", "patron")
senior <- c("senior", "first", "1st", "director" , "leader", "head", "chief", "master", "jefa", "jefe")
leader <- c("senior", "first", "1st", "director" , "leader", "head", "chief", "master", "jefa", "jefe", "manager")


df2 <- df1 %>%
  mutate(Job = ifelse((grepl(captain, CurrentRole, ignore.case = TRUE)) == TRUE &
                      (grepl(paste(engineer, collapse = "|"), CurrentRole, ignore.case = 
                      TRUE)) == TRUE ,"Allrounder",
               ifelse((grepl(captain, CurrentRole, ignore.case = TRUE)) == TRUE &
                      (grepl(paste(steward, collapse = "|"), CurrentRole, ignore.case = TRUE)) == TRUE ,
                      "Allrounder",
               ifelse((grepl(captain, CurrentRole, ignore.case = TRUE)) == TRUE &
                      (grepl(paste(deckhand, collapse = "|"), CurrentRole, ignore.case = 
                      TRUE)) == TRUE ,"Allrounder",
               ifelse((grepl(captain, CurrentRole, ignore.case = TRUE)) == TRUE &
                      (grepl(paste(chef, collapse = "|"), CurrentRole, ignore.case = 
                      TRUE)) == TRUE ,"Allrounder", 
                      #---------------------------------------------------------
               ifelse((grepl(paste(engineer, collapse = "|"), CurrentRole, ignore.case = 
                      TRUE)) == TRUE & (grepl(paste(steward, collapse = "|"), CurrentRole, ignore.case = TRUE)) 
                      == TRUE , "Allrounder",
               ifelse((grepl(paste(engineer, collapse = "|"), CurrentRole, ignore.case = 
                      TRUE)) == TRUE & (grepl(paste(deckhand, collapse = "|"),  
                      CurrentRole, ignore.case = TRUE)) == TRUE ,"Allrounder",
               ifelse((grepl(paste(engineer, collapse = "|"), CurrentRole, ignore.case = 
                      TRUE)) == TRUE & (grepl(paste(chef, collapse = "|"), CurrentRole, 
                      ignore.case = TRUE)) == TRUE , "Allrounder", 
                      #--------------------------------------------------------
               ifelse((grepl(paste(steward, collapse = "|"), CurrentRole, ignore.case = TRUE)) == TRUE &
                      (grepl(paste(deckhand, collapse = "|"), CurrentRole, ignore.case = 
                      TRUE)) == TRUE , "Allrounder",
               ifelse((grepl("stew", CurrentRole, ignore.case = TRUE)) == TRUE &
                      (grepl(paste(chef, collapse = "|"), CurrentRole, ignore.case = 
                      TRUE)) == TRUE , "Allrounder", 
                      #--------------------------------------------------------
               ifelse((grepl(paste(deckhand, collapse = "|"), CurrentRole, ignore.case = 
                      TRUE)) == TRUE & (grepl(paste(chef, collapse = "|"), CurrentRole, 
                      ignore.case = TRUE)) == TRUE ,"Allrounder", 
                      
               # Next: assign jobs by finding key words in CurrentRole
               ifelse((grepl(captain, CurrentRole, ignore.case = TRUE)) == TRUE,"Captain",
               ifelse((grepl(paste(deckhand, collapse = "|"), CurrentRole, ignore.case = 
                     TRUE)) == TRUE,"Deckhand",
               ifelse((grepl(paste(steward, collapse = "|"), CurrentRole, ignore.case = TRUE)) == TRUE,"Steward",
               ifelse((grepl(paste(engineer, collapse = "|"), CurrentRole, ignore.case = 
                     TRUE)) == TRUE,"Engineer",
               ifelse((grepl(paste(chef, collapse ="|"), CurrentRole, ignore.case = 
                     TRUE)) == TRUE,"Chef",
               ifelse((grepl(paste(unemployed, collapse ="|"), CurrentRole, ignore.case = 
                     TRUE)) == TRUE, "Unemployed", "Other"))))))))))))))))) %>%
  mutate(HighPerformer = ifelse((grepl(paste(senior, collapse = "|"), CurrentRole, ignore.case = TRUE)) == TRUE, 1, 0),
         Leader = ifelse((grepl(paste(leader, collapse = "|"), CurrentRole, ignore.case = TRUE)) == TRUE, 1, 0)) %>%
  
  mutate(Job = as.factor(Job))
```

```{r, cache=TRUE, echo=FALSE, results=FALSE}
# Define industries:
airline <- c("line", "air", "wings", "ryanair", "easyjet", "jet", "fly", "flight")
yachting <- c("yach", "boat", "chart", "ship", "crew", "sail", "ocean", "farry",
              "coast", "cruise", "wave", "motor", "acht", "yac", "yate", "barco",
              "water", "nautic", "ferrie", "freight", "cargo", "royal caribbean",
              "10m", "20m", "30m", "40m", "50m", "46m", "55m", "100m", "107m", "M/Y",
              "catamaran")
hotel <- c("hotel", "hyatt", "hilton", "resort", "lux", "accom")
gastro <- c("restaur", "baker", "bar", "food", "ristorante", "caffe", "coffe",
            "steak", "cafe", "beef", "cake", "catering", "chef", "burger",
            "gastro", "Café", "cook", "breakfast", "mcdonald", "resto")
defence <- c("minist", "defenc", "marine", "navy", "force", "army", "marit",
             "safety", "security", "red cross", "milit")
freelance <- c("freelance", "self", "free lance")
# chains <- c("ryanair", "walmart", "carrefour", "easyjet")


df3 <- df2 %>%
  mutate(Industry = 
           ifelse((grepl(paste(yachting, collapse = "|"), CurrentCompany, ignore.case = 
           TRUE)) == TRUE,"yachting",
           ifelse((grepl(paste(gastro, collapse = "|"), CurrentCompany, ignore.case = 
           TRUE)) == TRUE,"gastro",
           ifelse((grepl(paste(hotel, collapse = "|"), CurrentCompany, ignore.case = 
           TRUE)) == TRUE,"hotel",
           ifelse((grepl(paste(defence, collapse = "|"), CurrentCompany, ignore.case = 
           TRUE)) == TRUE,"defence",
           ifelse((grepl(paste(freelance, collapse = "|"), CurrentCompany, ignore.case = 
           TRUE)) == TRUE,"freelance",
           ifelse((grepl(paste(airline, collapse = "|"), CurrentCompany, ignore.case = 
           TRUE)) == TRUE,"airline", "Other"))))))) %>%
  mutate(Industry = as.factor(Industry))
```

\subsubsection{Extending the Dataset}

Some of the identified issues of the dataset are that the number of relevant predictors is quite low and that the set does not include a lot of continuous variables. Therefore, extending the dataset with additional data might be beneficial for the performance of our models.
In order to extend the dataset with further predictors, we identified the variable \textit{Country} as the most suitable connecting factor: It presents the possibility to investigate whether or not specific characteristics of the candidate’s country of origin contribute to the star rating. 
Subsequently, we gathered data into the csv [additional_data.csv](./additional_data.csv), which adds the following factors to every country:
\begin{itemize}
\item \textit{Region}: We decided to group the countries into the 4 Main economic sales regions EMEA (Europe, Middle East and Africa), North America, LATAM (Latin America) and APAC (Asia and Pacific)
\item \textit{Pop}: Total population \cite{worldometers}
\item \textit{Coastline}: The Coastline of a country in km. Source of this data is the World Resources Institute \cite{WRI}
\item \textit{CoastRatio}: The ratio of coastline per land area of a country (in meters coastline per square kilometer land area) \cite{WRI}
\item \textit{Happiness (Report)}: A country’s score in the World Happiness Report in 2019. \cite{WHR} This report does not include all countries, therefore we later inserted the economic region’s average for NA values
\item \textit{GDPpC}: Gross Domestic Product of a country divided by the population (data from 2020) \cite{WorldGDP}
\end{itemize}

By matching the country name in this table to the country name used by the M Yachts Data, we were able to join the country data to the dataset. 

In a second step, we repeated the same process with Tourism Data from the World Bank Group \cite{WDI}, since we believe that the extent of a country’s tourism sector could have an effect on the quality of their yachting services. We added three further variables to the dataset gathererd in the csv [tourism_data_edited.csv](./tourism_data_edited.csv):
\begin{itemize}
\item \textit{Tourists}: Number of arriving tourists per year
\item \textit{Price level}: Purchasing power parity (PPP) adjusted price level
\end{itemize}
To smooth out short-term fluctuations we took an average value of the years 2019,2020 and 2021 for each variable. 

These new variables present the opportunity to look at the potential circumstances of a candidate regarding the yachting industry. We chose the new variables by potential influence on the performance of a country’s inhabitants on their ability to work in yachting. For example, it is possible that countries with a higher ratio of coastline are more suited for bringing up excellent yachting employees.

```{r, echo=FALSE, results=FALSE}
# Read in addional data frame
# Rename variables
# Convert to american numbers format
# Merge with data from Myachts
additional_data <- read.csv("additional_data.csv", na.strings = c("", "NA", "N/A", "n/a"), sep=";") %>%     
    rename(Region = Economic.Region,
         Pop = Population..2020.,
         Coastline = Coastline.WRI..km.,
         CoastRatio = Coast.Area.Ratio..WRI...m.km2.,
         Happiness = Happiness.Report) %>%
  
  mutate(Region = as.factor(Region),
         Country = as.factor(Country),
         Pop = as.numeric(stri_replace_all_regex(Pop,
                                  pattern=c("\\.", ","),
                                  replacement=c("", "."),
                                  vectorize=FALSE)),
         Happiness = as.numeric(stri_replace_all_regex(Happiness,
                                  pattern=c("\\.", ","),
                                  replacement=c("", "."),
                                  vectorize=FALSE)),
         Coastline = as.numeric(stri_replace_all_regex(Coastline,
                                  pattern=c("\\.", ","),
                                  replacement=c("", "."),
                                  vectorize=FALSE)),
         CoastRatio = as.numeric(stri_replace_all_regex(CoastRatio,
                                  pattern=c("\\.", ","),
                                  replacement=c("", "."),
                                  vectorize=FALSE))) %>%
  select(Country, Region, Pop, Happiness, Coastline, CoastRatio)

# summary(additional_data)

# Merge data
df5 <- inner_join(df3, additional_data, by = "Country")
df6 <- na.omit(df5)
```

```{r, echo=FALSE, results=FALSE}
# Add new variables such as tourism (obtained from WDI)
# Merge with previous dataset
Tourism_df <- read.csv("tourism_data_edited.csv", na.strings = c("..", ""), sep=",") %>%
  rename(Country = Country.Name,
         Year = Time,
         GDPpC = GDP.per.capita..PPP..constant.2017.international.....NY.GDP.PCAP.PP.KD.,
         Tourists = International.tourism..number.of.arrivals..ST.INT.ARVL.,
         Price_level = Price.level.ratio.of.PPP.conversion.factor..GDP..to.market.exchange.rate..PA.NUS.PPPC.RF.) %>%
  mutate(GDPpC = gsub(",",".",GDPpC),
         Tourists = gsub(",",".",Tourists),
         Price_level = gsub(",",".",Price_level),
         Country = as.factor(Country),
         GDPpC = as.numeric(GDPpC),
         Tourists = as.numeric(Tourists),
         Price_level = as.numeric(Price_level)) %>%
  select(Country, Year, GDPpC, Tourists, Price_level)

# Build three year averages of new predictors to ensure more reliable values
Tourism_avg <- Tourism_df %>%
  group_by(Country) %>%
  summarise(meanGDPpC = mean(GDPpC, na.rm = TRUE),
            meanPriceLevel  = mean(Price_level, na.rm = TRUE),
            meanTourists  = mean(Tourists, na.rm = TRUE)) %>%
  select(Country, meanGDPpC, meanPriceLevel, meanTourists)

# Merge Tourism data with previous dataset
df7 <- inner_join(df6, Tourism_avg, by = "Country")
setdiff(df5$Country, Tourism_avg$Country)
```

```{r, echo=FALSE, results=FALSE}
# Reduce number of countries (if n(Country) < 10 --> "Other")
frequent_countries <- df6 %>%
  group_by(Country) %>%
  count()

df8 <- left_join(df7, frequent_countries, by = "Country")

df9 <- df8 %>%
  mutate(Country_edited = ifelse(n < 10, "Other", as.character(Country)),
         Country_edited = as.factor(Country_edited),
         Tourists_pop = meanTourists / Pop,
         Tourists_coast = meanTourists / Coastline) %>%
  select(Job, StarRating, Industry, Country_edited,
         Region, Happiness, Coastline, CoastRatio, TalentSource, Pop, 
         ExperienceYears, CvStrength, HighPerformer, Leader,
         meanGDPpC, meanTourists, Tourists_coast, Tourists_pop, meanPriceLevel)

#Remove NAs
df10 <- na.omit(df9)
```

```{r}
#Removing outliers
#Write function for outliers
outliers <- function(x) {

  Q1 <- quantile(x, probs=.25)
  Q3 <- quantile(x, probs=.75)
  iqr = Q3-Q1

 upper_limit = Q3 + (iqr*1.5)
 lower_limit = Q1 - (iqr*1.5)

 x > upper_limit | x < lower_limit
}

remove_outliers <- function(df, cols = names(df)) {
  for (col in cols) {
    df <- df[!outliers(df[[col]]),]
  }
  df
}
#Apply function to dataframe
df10 <- remove_outliers(df10, c("ExperienceYears"))

#Control new length of dataframe
nrow(df10)
```
\subsection{Exploratory Data Analysis (EDA)}

EDA involves analyzing and summarizing the characteristics of a dataset, identifying patterns and anomalies, and visualizing the data to gain insights. This helps to understand the given data and identify any potential issues that may affect the accuracy of the used models. 

-> end with an outlook: In summary, the dataset does not appear perfectly suited for the prediction task. First of all, there is a There are only few continuous variables that can be directly attributed to the 

#Exploratory data analysis
```{r, echo=FALSE}
basic_eda <- function(data)
{
  glimpse(data)
  print(status(data))
  freq(data) 
  print(profiling_num(data))
  plot_num(data)
  describe(data)
}

basic_eda(df10)
```

#Check correlations
```{r, echo=FALSE}
library(corrplot)
df11 <- df10%>%
  select(StarRating, meanGDPpC, Happiness, ExperienceYears, CvStrength, Pop, 
         HighPerformer, Leader, Coastline, CoastRatio, meanTourists, meanPriceLevel,
         Tourists_pop, Tourists_coast)
  
  ggcorr(df11, label = TRUE)
  
```

#Highest rating: Engineer and Captain
#Lowest rating: Unemployed
--------------------------------------
#Highest experience: Captain, Engineer
#Lowest experience: Deckhand
--------------------------------------
#Highest CV score: Engineer
#Lowest CV score: Steward
--------------------------------------
#Rating and experience are moderately correlated (0.45)
```{r, echo=FALSE}
#StarRating per job category
#Highest rating: Engineer and Captain
#Lowest rating: Unemployed
n_job <- df10 %>%
  count(Job)

df10$StarRating <- as.numeric(df10$StarRating)

descriptive_rating_per_job <- df10 %>%
  group_by(Job) %>%
  summarise(meanRating = mean(StarRating),
            minRating = min(StarRating),
            maxRating = max(StarRating),
            sdRating = sd(StarRating)) %>%
  arrange(desc(meanRating)) %>%
  select(Job, meanRating, minRating, maxRating, sdRating)

(rating_per_job <- left_join(descriptive_rating_per_job, n_job, by= "Job"))

#Visualize
ggplot(rating_per_job, aes(Job, meanRating))+
  geom_bar(stat = "identity")

ggplot(df10, aes(Job, StarRating))+
  geom_boxplot()

#Experience per job category
#Highest experience: Captain, Engineer
#Lowest experience: Deckhand
descriptive_exp_per_job <- df10 %>%
  group_by(Job) %>%
  summarise(meanExp = mean(ExperienceYears),
            minExp = min(ExperienceYears),
            maxExp = max(ExperienceYears),
            sdExp = sd(ExperienceYears)) %>%
  arrange(desc(meanExp)) %>%
  select(Job, meanExp, minExp, maxExp, sdExp)
(exp_per_job <- left_join(descriptive_exp_per_job, n_job, by="Job"))

#CvStrength per job category
#Highest: Engineer
#Lowest: Steward
descriptive_cv_per_job <- df10 %>%
  group_by(Job) %>%
  summarise(meanCvStrength = mean(CvStrength),
            minCvStrength = min(CvStrength),
            maxCvStrength = max(CvStrength),
            sdCvStrength = sd(CvStrength)) %>%
  arrange(desc(meanCvStrength)) %>%
  select(Job, meanCvStrength, minCvStrength, maxCvStrength, sdCvStrength)
(cv_per_job <- left_join(descriptive_cv_per_job, n_job, by="Job"))

#Find correlations for jobs
join1 <- inner_join(rating_per_job, descriptive_exp_per_job, by = "Job")
(total_job_ratings <- inner_join(join1, descriptive_cv_per_job, by = "Job"))
#Rating and experience are moderately correlated (0.45)
cor(total_job_ratings$meanRating,total_job_ratings$meanExp)
cor(total_job_ratings$meanRating, total_job_ratings$meanCvStrength)
cor(total_job_ratings$meanExp, total_job_ratings$meanCvStrength)


```
#Rating of high performer lower than no high performer
#same for leader
```{r, echo=FALSE}
#Mean rating of high performer
descriptive_high_performer <- df10 %>%
  group_by(HighPerformer) %>%
  summarise(meanRating = mean(StarRating)) %>%
  arrange(desc(meanRating)) %>%
  select(HighPerformer, meanRating)

#Count number of HighPerformer
n_hp <- df10 %>%
  count(HighPerformer)
(rating_high_performer <- inner_join(descriptive_high_performer, n_hp, by = "HighPerformer"))


#Mean rating of leader
descriptive_leader <- df10 %>%
  group_by(Leader) %>%
  summarise(meanRating = mean(StarRating)) %>%
  arrange(desc(meanRating)) %>%
  select(Leader, meanRating)

#Count number of leader
n_leader <- df10 %>%
  count(Leader)
(rating_leader <- inner_join(descriptive_leader, n_leader, by = "Leader"))

```

#Same number of observations in each class of StarRating
```{r}
#Distribution of rankings
distr_of_ratings <- df10 %>%
  mutate(StarRating = as.factor(StarRating))
summary(distr_of_ratings$StarRating)
```
#Short: Candidates from yachting industry have superior rating, exp and cv score
#Highest rating in         yachting industry
#Highest CvStrength in     yachting industry
#Highest experience have   freelancers (2. defence, 3. yachting)
```{r}
#Count number of industry
n_indusry <- df10 %>%
  count(Industry)

#Rating per industry
descriptive_industry_rating <- df10 %>%
  group_by(Industry) %>%
  summarise(meanRating = mean(StarRating)) %>%
  arrange(desc(meanRating)) %>%
  select(Industry, meanRating)
(rating_industry <- inner_join(descriptive_industry_rating, n_indusry, by = "Industry"))

#Experience per industry
descriptive_industry_exp <- df10 %>%
  group_by(Industry) %>%
  summarise(meanExp = mean(ExperienceYears)) %>%
  arrange(desc(meanExp)) %>%
  select(Industry, meanExp)
(exp_industry <- inner_join(descriptive_industry_exp, n_indusry, by = "Industry"))

#CvStrength per industry
descriptive_industry_cv <- df10 %>%
  group_by(Industry) %>%
  summarise(meanCV = mean(CvStrength)) %>%
  arrange(desc(meanCV)) %>%
  select(Industry, meanCV)
(rating_industry <- inner_join(descriptive_industry_cv, n_indusry, by = "Industry"))
```

#Short: Experience in yachting seems to increase the rating over all jobs 
#       except for engineers
#Best rating for chefs with experience in yachting
#Best rating for captains with experience in freelance, defence, yachting
#Best rating for deckhand with experience in yachting (place 1 ignored since n=1)
#Best rating for steward with experience in airline, yachting
#Best rating for chefs with experience in yachting (place 1 ignored since n=1)
#Best rating for engineers with experience in "Other"
#Best rating for others with experience in hotel and yachting
```{r}
#Are experience in yachting industry beneficial for every job?
job_and_industry <- df10 %>%
  group_by(Job, Industry) %>%
  summarise(meanRating = mean(StarRating)) %>%
  arrange(desc(meanRating)) %>%
  select(Job, Industry, meanRating)

n_job_and_industry <- df10 %>%
  group_by(Job) %>%
  count(Industry)

rating_job_industry <- inner_join(job_and_industry, n_job_and_industry, by = c("Job", "Industry"))

#Chefs with yachting experience have better ratings
rating_job_industry %>%
  filter(Job == "Chef")

#Engineer's rating does not depend on industry expertise
rating_job_industry %>%
  filter(Job == "Engineer")

#Most captains have yachting experience as expected (many others couldn't be filtered)
rating_job_industry %>%
  filter(Job == "Captain")

#Yachting experience beneficial for rating
rating_job_industry %>%
  filter(Job == "Deckhand")

#Same as Deckhand, but best rating with airline experience
rating_job_industry %>%
  filter(Job == "Steward")

#Yachting industry again beneficial
rating_job_industry %>%
  filter(Job == "Allrounder")

#For unknown job categories, the best ratings are found in hotel and yachting
rating_job_industry %>%
  filter(Job == "Other")

```

#Best avg. rating in: Saudi Arabia, South Africa, Phillipines
#Results unstable since n is dominated by Spain with 273 obs. <---------
#e.g.: Best captains come from Germany

```{r}
#Rating per country
descriptive_country_rating <- df10 %>%
  group_by(Country_edited) %>%
  summarise(meanRating = mean(StarRating),
            minRating = min(StarRating),
            maxRating = max(StarRating),
            sdRating = sd(StarRating)) %>%
  arrange(desc(meanRating)) %>%
  select(Country_edited, meanRating, minRating, maxRating, sdRating)

n_country <- df10 %>%
  count(Country_edited)

(rating_countries <- inner_join(descriptive_country_rating, n_country, by= "Country_edited"))

#Job & Country
job_and_country <- df10 %>%
  group_by(Job, Country_edited) %>%
  summarise(meanRating = mean(StarRating)) %>%
  arrange(desc(meanRating)) %>%
  select(Job, Country_edited, meanRating)

n_job_and_country <- df10 %>%
  group_by(Job) %>%
  count(Country_edited)

rating_job_country <- inner_join(job_and_country, n_job_and_country, by = c("Job", "Country_edited"))
rating_job_country <- rating_job_country %>%
  filter(n > 2)
```

\section{Methods}

To really understand what machine learning methods are used and how they work, it is important to first understand what machine learning is: 
Machine learning is a subfield of artificial intelligence that involves the development of algorithms and models that can learn from and make predictions or decisions based on data. These algorithms and models are able to improve their performance over time as they are exposed to more data.

In machine learning, algorithms are trained on a dataset, which consists of a collection of examples that include input data and the corresponding correct output. The algorithm uses this training data to learn how to map the input data to the correct output. Once the algorithm has learned from the training data, it can then be applied to new, unseen data to make predictions or decisions. In the following, we split the wrangled data with the odds \textit{80:20} into training and testing data, respectively.

```{r , cache = TRUE, echo=FALSE, message=FALSE, results=FALSE}
df10$StarRating <- as.factor(df10$StarRating)
set.seed(1234)
df_split <- initial_split(df10 , prop = 0.8)
df_train <- training(df_split)
df_test <- testing(df_split)
```

There are several different types of machine learning, including supervised learning and unsupervised learning. Supervised learning involves training an algorithm on labeled data, where the correct output is provided for each example in the training data. Unsupervised learning comprises training an algorithm on unlabeled data, where the algorithm must discover the underlying structure of the data on its own.

\subsection{Supervised Learning}

In supervised learning, the training data consists of a set of input-output pairs. The algorithm uses these pairs to learn the relationship between the input and the output. Once the algorithm has learned this relationship, it can then be applied to new, unseen input data to predict the corresponding output.

Examples of supervised learning tasks include classification, where the goal is to predict a class label for a given input data, and regression, where the objective is to predict a continuous output value.

Supervised learning is widely used in many applications, such as image classification, spam detection, and stock price prediction. It is a powerful tool for solving problems that require the prediction of a specific output value based on input data.

To improve the performance of the different models cross-validation is used during for each of the following models. 

Cross-validation is a resampling procedure used to evaluate the performance of a machine learning model. It involves dividing the dataset into k subsets, training the model on k-1 subsets, and evaluating it on the remaining subset. This process is then repeated k times, with a different subset being used as the evaluation set in each iteration. The final performance metric is then calculated as the average of the performance across all k iterations.

An example for how to build a model in R from scratch, including hyperparameter tuning and using cross validation will be shown in section 3.1.3, as XGBoosting is one of our most complex models.

\subsubsection{Classification}

In classification, the input data is usually represented as a set of features, which are attributes or characteristics of the data.
The class label is the output that the algorithm is trying to predict. For example, in a binary classification problem, there are only two possible class labels (e.g., spam or not spam). In a multi-class classification problem, there are more than two possible class labels.

The goal of the paper is to predict the star rating of the given data set which can be interpreted as a multi-class classification with score from zero to five. Thus, different classification methods could be a good approach to find an answer to the research question. 
While the research problem could also be interpreted as a regression problem, classification algorithms seem more intuitive and suitable to solve it, which is why a major part of the model selection will deal with classification and only a minor part will deal with regression methods.
In the following section we are going to discuss the suitability of common classification methods and how to code them in R.

\subsubsection{Decision Tree}

A decision works by creating a tree-like model of decisions and their possible consequences. The tree is created by splitting the data into smaller and smaller subsets based on certain feature values. At each node in the tree, the model makes a decision based on the value of a certain feature and directs the data to the right branch of the tree. The final decision at the end of the tree is the predicted outcome.

Decision trees are easy to understand and interpret, and they can handle both continuous and categorical data. However, they can be prone to overfitting and may not always perform as well as other machine learning algorithms.

The tree depth is the main parameter in a decision tree. It can be determined by setting a minimum number of data points for a node to get split further, a maximum tree depth, or can be driven by a cost-complexity parameter (used in this application). The cost-complexity parameter is another way to prune the tree, so that it controls the trade-off between the tree’s complexity and accuracy, in other words the trade-off between bias and variance. It is chosen by cross validation to find the value that results in the best performance on the test data.

```{r Decision Tree, cache=TRUE, echo = FALSE, message = FALSE}

class_tree_spec <- decision_tree() %>%
  set_engine( "rpart" ) %>% 
  set_mode("classification")

#create recipe
class_tree_recipe <- 
  recipe( formula = StarRating ~ ., data = df_train ) %>% 
  step_novel( all_nominal_predictors() ) %>% 
  step_dummy( all_nominal_predictors() ) %>% 
  step_zv( all_predictors() )

#create workflow
class_tree_wf <- workflow() %>%
  add_model( class_tree_spec %>% set_args(cost_complexity = tune() )) %>%
  add_recipe( class_tree_recipe )

# K-fold cross validation
df_fold <- vfold_cv( df_train )

# Grid with values
param_grid <- grid_regular( cost_complexity( range = c( -3, -1 )), levels = 10 )

# Tuning
tune_res <- tune_grid(
  class_tree_wf, 
  resamples = df_fold, 
  grid = param_grid, 
  metrics = metric_set( accuracy )
)

# Select best value
best_complexity <- select_best( tune_res )

# Finalize workflow with best value
class_tree_final <- finalize_workflow( class_tree_wf, best_complexity )

# Fit final model
class_tree_final_fit <- fit( class_tree_final, data = df_train )

best_complexity
```

\subsubsection{Extreme Gradient Boosting XGBoosting}

The XGBoosting algorithm was invented by Tianqi Chen in 2014 and is one of the most used machine learning methods nowadays. It is an implementation of gradient boosting to make the algorithm more efficient and accurate.

To understand the XGBoosting algorithm it is necessary to understand the basic concepts of boosting and its specific type gradient boosting. Boosting is a method of combining a set of weak learners into a single strong learner. Boosting works by training weak learners sequentially, with each learner being trained to correct the mistakes made by the previous learner. The final model is a weighted sum of the weak learners, with the weights determined by the performance of each learner on the training data.[ISLR]

Gradient boosting is a specific type of boosting, which uses gradient descent instead of the two-step process explained above for training the weak learners. The key adjustments of XGBoosting are that it uses advanced regularisations and parallel-processing. This makes the model more robust to overfitting and improves its efficiency.

As extreme gradient boosting is one of our most complex models the following part will describe, how to set up an XGBoost algorithm in R, also as an example for all other methods.

First, we are specifying the model. To do that, we are using the \textit{boost_tree()} function of the \textit{parsnip} package. To classify that we are using extreme gradiant boosting, the engine is set to \textit{xgboost} and as we are using it as a classifier the mode is set to \textit{classification}. XGBoost requires many arguments to be set, we are choosing the following hyperparameters by tuning with cross validation:
\begin{itemize}
\item \textit{trees}≙ An integer for the number of trees contained in the ensemble
\item \textit{tree_depth}≙ An integer for the maximum depth of the tree
\item \textit{min_n}≙ An integer for the minimum number of data points in a node that is required for the node to be split further
\item \textit{loss_reduction}≙ A number for the reduction in the loss function required to split further
\item \textit{sample_size}≙ A proportion of data that is exposed to the fitting routine, which is done at each iteration
\item \textit{mtry}≙ A proportion of predictors that will be randomly sampled at each split when creating the tree models
\item \textit{learn_rate}≙ A number for the rate at which the boosting algorithm adapts from iteration-to-iteration
\end{itemize}
```{r, cache=TRUE}
xgb_spec <- boost_tree(
  trees = tune(), 
  tree_depth = tune() , min_n = tune(), 
  loss_reduction = tune(),                     
  sample_size = tune(), mtry = tune(),         
  learn_rate = tune(),                         
) %>% 
  set_engine( "xgboost" ) %>% 
  set_mode( "classification" )
```

Second, a grid of hyperparameter values for our XGBoost model using the \textit{grid_latin_hypercube()} function from the \textit{tune} package is defined. The \textit{grid_latin_hypercube()} function generates a grid of hyperparameter values using Latin Hypercube sampling, which is a method of randomly sampling a set of points such that each dimension is evenly covered:
```{r, cache=TRUE}
xgb_grid <- grid_latin_hypercube(
  trees(),
  tree_depth(),
  min_n(),
  loss_reduction(),
  sample_size = sample_prop(),
  finalize(mtry(), df_train),
  learn_rate(),
  size = 100
)
```

Third, we are creating a recipe for preprocessing data. Here \textit{StarRating} is our output variable and all other variables are our features. We are training the algorithm on the training data set \textit{df_train}. The \textit{step_novel()} function is used to identify which of the nominal predictor variables have not been seen in the training data before. These "novel" levels will be treated as a separate category when the recipe is baked. The \textit{step_dummy()} function creates dummy variables for all of the nominal predictor variables. The \textit{step_zv()} function standardizes all of the predictor variables by subtracting the mean and dividing by the standard deviation: 
```{r, cache=TRUE}
xgb_recipe <- 
  recipe( formula = StarRating ~ ., data = df_train ) %>% 
  step_novel( all_nominal_predictors() ) %>% 
  step_dummy( all_nominal_predictors() ) %>% 
  step_zv( all_predictors() )
```

As a next step we are defining our workflow and setting up 10-fold cross validation to improve the model performance:
```{r, cache=TRUE}
xgb_wf <- workflow() %>%
  add_recipe( xgb_recipe ) %>%
  add_model( xgb_spec )
df_fold <- vfold_cv(df_train, 10)
```

To tune the model we are using the \textit{doParallel::registerDoParallel()} function to set up parallel processing. This can be used to speed up the training and evaluation process by allowing multiple models to be trained and evaluated at the same time.
The \textit{tune_grid()} function will train and evaluate our XGBoost model with each combination of hyperparameters in the \textit{xgb_grid} object, using the resamples from cross validation defined above:
```{r, cache=TRUE}
doParallel::registerDoParallel()
xgb_res <- tune_grid(
  xgb_wf,
  resamples = df_fold,
  grid = xgb_grid,
  control = control_grid(save_pred = TRUE)
)
```


Next, we are selecting the best combination of hyperparameter values to reach the highest accuracy, by using the \textit{select_best()} function and setting the metric to "accuracy":
```{r, cache=TRUE}
best_acc <- select_best( xgb_res, "accuracy" )
best_acc
```

Final, the workflow gets finalised by adding the optimal hyperparameter values to the predefined workflow and fitting the model to the training data set \textit{df_train}:
```{r, cache=TRUE}
xgb_final <- finalize_workflow(
  xgb_wf,
  best_acc
)
xgb_final_fit <- fit( xgb_final, data = df_train )
```


\subsubsection{Random Forest}

The random forest algorithm is one of the most famous and widely used machine learning algorithms nowadays. It is an ensemble method, which means that the algorithm combines the predictions of multiple individual models to make a final prediction. In a random forest, these individual models are decision trees.

To construct a random forest, many decision trees are trained on bootstrapped samples of the training data and a random subset of the features. The final prediction is made by averaging the predictions of all the individual decision trees. This averaging process helps to reduce the variance and improve the overall performance of the model. 

As the random forest classifier can handle both continuous and categorical features and is able to predict multiple classes as an output, it is perfectly suited for the decision problem. To run the random forest algorithm there are a few hyperparameters that can be chosen upfront or tuned. The main hyperparameters, when running a random forest algorithm in R are the number of trees the algorithm includes, the number of predictors randomly sampled at each split and the minimum number of data points in a tree node required to split the node further. We predefined the number of trees at 1000, as there is no big increase in the model performance with tuning the number of trees and it takes high computational costs.

In this application the hyperparameters are tuned using 10-fold cross validation. 
This means the data is split into 10 folds, and the model is trained and evaluated on different subsets of the training data. This allows for a more robust evaluation of the model’s performance, as it is tested on a variety of different data points. The final tuning results are shown beneath:
```{r RandomForest, cache=TRUE, echo=FALSE,message=FALSE}
set.seed(1234)
# Create recipe
rf_recipe <- 
  recipe( formula = StarRating ~ ., data = df_train ) %>% 
  step_novel( all_nominal_predictors() ) %>% 
  step_dummy( all_nominal_predictors() ) %>% 
  step_zv( all_predictors() )
# Tune specifications and set number of trees to 1000
rf_tune_spec <- rand_forest(
  mtry = tune(),
  trees = 1000,
  min_n = tune()
) %>%
  set_mode( "classification" ) %>%
  set_engine( "randomForest" )

# Create workflow
rf_tune_wf <- workflow() %>%
  add_recipe( rf_recipe ) %>%
  add_model( rf_tune_spec )

# Do cross validation
df_fold <- vfold_cv( df_train,10 )

# Create a more specific grid with setting the mtry between 2 and 13 
rf_grid <- grid_regular(
  mtry(range = c (2,13)), 
  min_n(),
  levels = 20
)

# Tune again
doParallel::registerDoParallel()
rf_regular_res <- tune_grid(
  rf_tune_wf,
  resamples = df_fold,
  grid = rf_grid
)

# Select best a value
best_acc <- select_best( rf_regular_res, metric = "accuracy" )
best_acc
# Finalize the workflow with best value
rf_final <- finalize_workflow( rf_tune_wf, best_acc )

# Fit final model
rf_final_fit <- fit( rf_final, data = df_train )
```

\subsubsection{Naive Bayes}

In the context of machine learning, naive Bayes can be used to classify data points based on their features. The algorithm makes the assumption that the features are independent of one another, which is known as the "naive" assumption. This assumption simplifies the calculations and allows the algorithm to be implemented efficiently.

To classify a new data point, the algorithm calculates the probability of the data point belonging to each class. The class with the highest probability is chosen as the prediction for the data point. The probability is calculated based on the prior probability of the class and the likelihood of the features given the class. 

```{r Naive Bayes, cache=TRUE, echo = FALSE, message=FALSE, results=FALSE}
set.seed(42)

#Tuning
nb_grid <-   expand.grid(usekernel = TRUE,
                         laplace = c(0, 0.5, 1, 1.5, 2, 2.5), 
                         adjust = c(0.75, 1, 1.25, 1.5))
#Run model
naive_bayes_via_caret <- train(StarRating ~ ., data = df_train, 
                               method = "naive_bayes",
                               usepoisson = TRUE,
                               tuneGrid = nb_grid)
#Model output
naive_bayes_via_caret

# Visualize the tuning process
plot(naive_bayes_via_caret)

# Perform classification 
predictions <- data.frame(predict(naive_bayes_via_caret, newdata = df_test))
comp_pred_true <-table(predictions, df_test$StarRating)
```

\subsubsection{K-nearest-neighbours}

The k-nearest neighbour (KNN) classifier is a simple and effective method for classification. In brief summary it works by finding the k data points in the training set that are closest to the input data (by a chosen metric), and then it uses those k data points to determine the output value, by choosing the class the most k neighbours are part of. One of the benefits of the KNN algorithm is that it is simple and easy to implement. It also does not make any assumptions about the underlying data distribution, so it can handle non-linear relationships well.

The algorithm can be sensitive to the choice of k and the distance metric. The provided implementation in R uses the Euclidean distance by default. We choose to use a for loop to iterate over different integer values of k (from 1 to 30) to plot the following elbow plot and chose the optimal k value.

```{r KNN, cache=TRUE, echo=FALSE , message=FALSE}
# Create recipe for KNN and normalize the numeric predictors
rec_spec <- recipe( StarRating ~ . , data= df_train ) %>%
  step_normalize( all_numeric_predictors() )

#Define the range k and create empty vectors for the for loop
  k_range <- 1:30
  predictions <- NA
  test_acc <- NA

#Creating a for loop do evaluate the model performance for different values of k
for (i in k_range) {
  k <- k_range[i]
  knn_spec1 <- nearest_neighbor(neighbors = k) %>%
    set_mode("classification") %>%
    set_engine("kknn")
  lm_wf1 <- workflow() %>%
    add_model( knn_spec1 ) %>%
    add_recipe( rec_spec )
  knn_m <- lm_wf1 %>% fit(df_train)
  predictions[k] <- predict(knn_m, df_test)
  test_acc[k] <- mean(predictions[[k]] == df_test$StarRating)
}

#create a data frame with the different values of k and the underlying accuracies on the test data set
df_acc_knn <- data.frame("k"=c(1:30),test_acc)

#plot the test accuracy against the differnet values of k
ggplot(df_acc_knn, aes(k, test_acc))+geom_line()+ggtitle("Elbow Plot") 

```

\subsubsection{Other Classification Algorithms}

Another algorithm that can be used for classification is a neural network, often described with the term “Deep Learning”. A neural network is a type of machine learning model that is inspired by the structure and function of the brain. It consists of a large number of interconnected processing units (neurons) that are organised into layers, and it is trained using a large dataset and an optimization algorithm. Neural networks are able to learn complex relationships between input and output data. 

While a neural network is generally well suited for multi-class classification, the performance of a neural network will depend on the size and quality of the training dataset. Since our dataset is rather small and of medium to low quality, the neural network may not be able to learn the necessary relationships and may not perform well. Furthermore, the interpretability of a neural network is very small, since it uses a “black-box approach”, so results of this model might not be as helpful for us and MYachts as results of other classifiers like decision trees. Since a Neural Network is also hard to tune and optimise, we decided not to use it on the dataset.

Another classification algorithm is the Support Vector Machine (SVM). In its base form, SVM’s divide a dataset into two classes using a hyperplane that maximises the separation of the data points to their potential classes in an n-dimensional space. The data points with the smallest distance to the hyperplane are called Support Vectors. 
We decided against this algorithm because it is difficult to apply to multiclass classification problems - The usual way to achieve this is to divide the multiclass problem into multiple binary classifications, which is beyond the scope of this paper.

\subsubsection{Regression}

While classification algorithms are used to predict a discrete class label for a given input, regression algorithms predict a continuous output value. 
So far we looked at classification algorithms, because the star rating is in itself discrete - but the way a star rating is given leaves room for the possible use of regression algorithms:
For example, not every 2-star review is as good as another. A reviewer might see a profile as a high 2-star or a low 2-star profile. Therefore it is possible to interpret our outcome variable as continuous, just with a rounded result to derive at the next closest full number. 

Both classification algorithms and regression algorithms might lead to valid results in this case and it is therefore sensible to also explore regression methods.

When performing regression methods in this context, we could measure them on mean standard error (MSE), which is a measure of difference between the prediction and the actual star rating. One advantage of this approach is that it penalises very wrong predictions (e.g. a 1 star prediction on a 5 star candidate) more heavily. On the other hand we either have to round our predictions to the next full number, or have to accept that there’s always gonna be differences between prediction and truth, because only the prediction allows for decimal values. Furthermore, a regression model might also predict values below 0 or above 5 for extreme cases, which is not a possibility in a real-life context. 
To compare the performance better to the performance of classification methods, we can also choose to assess model success using accuracy, by rounding our results to the nearest full number and checking whether or not that is the correct rating. 

The most basic kind of regression is linear regression: it is a method used to model the linear relationship between a dependent variable and one or more independent variables. Generally it is used to model relationships between continuous variables, rather than categorical variables, which poses a problem for our predominantly categorical dataset. 
The method can be used with categorical variables, but the variables need to be transformed into numerical form in order for the model to be able to work with them. One way to do this is to create dummy variables for each category. A dummy variable is a binary variable that takes on the value of 1 if the observation belongs to a particular category, and 0 otherwise. Of course this is challenging for variables like “Country”, which has more than 100 factors, requiring us to create more than 100 dummy variables for just this one predictor. 
If we want to use regression methods, it might make sense to drop the factors with most levels entirely, since their singular dummy variables provide very little explanative value. 

Ridge regression and lasso regression are two techniques that can be used to improve the performance of linear regression models. These techniques are known as regularisation methods, because they add a penalty term to the model's objective function to help reduce overfitting and improve the model's generalisation to new data.

\paragraph{Linear Regression}

To summarise, regression methods generally are not perfectly suited for our dataset due to the nature of the many multi-level categorical variables, however they might yield a benefit in the interpretation by allowing decimal values.

\subsection{Unsupervised Learning}

Unsupervised learning is a type of machine learning in which the model is not given any labeled training data. Instead, the model must find patterns and relationships in the data on its own. This is in contrast to supervised learning, in which the model is given labeled training data and learns to make predictions based on that data.

Unsupervised learning is useful for exploring and understanding the structure of a dataset, and it can be used to generate new features or to identify patterns that may not be apparent in the raw data. However, it is generally less accurate than supervised learning and requires more data to be effective.

The machine learning field can be divided into dimensionality reduction and clustering.

\subsubsection{Dimensionality Reduction}
Principal Component Analysis:

\subsubsection{Clustering}
\paragraph{K-Means}

\section{Results}
Model evaluation and results of the different models
Which algorithm/ML method has the best accuracy? 

Why do we choose it and what could we change? 


\subsection{Plots}

\section{Discussion}
What challenges did we face in the prediction challenge? 
Data lack potential
\subsection{Challenges}
\subsection{Ethics}
Ethics (country and crime rate as predictor) 
\subsection{Outlook}
How to improve the application process? 
\newpage
\section{Evaluation of examination results}

\begin{enumerate}

\item \textbf{Same grade: We confirm that each student from our group has contributed equally to the project.}


  \begin{tabular}{@{}p{7cm}p{5cm}@{}}
  \\
  \hrulefill \\
  (Name, matriculation No.) \\
  \\
  \hrulefill \\
  (Name, matriculation No.) \\
  \\
  \hrulefill \\
  (Name, matriculation No.) \\
  \\
  \hrulefill \\
  (Name, matriculation No.) \\
  \end{tabular}
\\


\item \textbf{Individual grades: We request individual grades for each student from our group.}

Please describe \textbf{in detail} the contributions of each group member in the following:


\textit{Student 1:}
    
  \begin{tabular}{@{}p{7cm}p{5cm}@{}}
     \\
     \hrulefill \\
      (Signature)
   \end{tabular}
 \newline 
 
\textit{Student 2:}
  
    \begin{tabular}{@{}p{7cm}p{5cm}@{}}
     \\
     \hrulefill \\
      (Signature)
   \end{tabular}
 \newline 
 
\textit{Student 3:}

    \begin{tabular}{@{}p{7cm}p{5cm}@{}}
     \\
     \hrulefill \\
      (Signature)
   \end{tabular}
\newline

\textit{Student 4:}
  
    \begin{tabular}{@{}p{7cm}p{5cm}@{}}
     \\
     \hrulefill \\
      (Signature)
   \end{tabular}


\end{enumerate}

\newpage
\section{Bibliography}

\printbibliography

\setlength{\parindent}{0em}



