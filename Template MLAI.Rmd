---
title: "Prediction Challenge"
subtitle: "In Maschine Learning and Artificial Intelligence"
author: "Justin Engelker, Philipp Rosenkranz, Fabian Günkel, Lennard Kothe, Paul Wieder"
date: "21.12.2022"
output:
  pdf_document:
    number_sections: yes
  html_document:
    df_print: paged
header-includes:
- \usepackage{graphicx}
- \usepackage{float}
- \usepackage{amsmath}
- \usepackage{dcolumn}
- \usepackage{parskip}
- \usepackage[backend=bibtex]{biblatex}
- \addbibresource{BibliographyPC.bib}
- \usepackage{xcolor}
---

```{r, echo=FALSE}
# Setup
rm(list=ls())
set.seed(1234)
options(scipen=10000)
select <- dplyr::select
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, message = FALSE, warning = FALSE)
# Load Libraries
library( tidyverse )
library( tidybayes )
library( funModeling ) 
library( scico ) 
library( ggrepel )
library( scico )
library( ggthemes )
library( patchwork )
library( haven )
library( tibble )
library( knitr )
library( tinytex )
library( sandwich )
library( lmtest )
library( stargazer )
library(tidymodels)
library(ISLR)
library(GGally)
library(broom)
library(dotwhisker)
library(performance)
library(funModeling)
library(sjPlot)
library(dplyr)
library(data.table)
library(stringi)
library(Hmisc)
library(skimr)
library(tm)

options(digits = 3)
```

\newpage
\tableofcontents

\newpage
\setlength{\parindent}{2em}
\baselineskip=1.5\baselineskip
\section{Introduction}

Over the past decade, the labour market has become increasingly tight. Companies have strong difficulties finding matching and convincing professionals and spend a big part of their investments in employee marketing and active sourcing. 
The tension on the labour market is also noticeable on the yachting and luxury shipping branch. M Yachts, a charter platform to book yachting experiences is highly affected by the described development. In addition, the yachting industry is affected by a high turnover of personnel working as crew members on boats and has very demanding and exclusive customers. For this reason, it is even more difficult and important to find suitable employees for companies in the yachting industry such as M Yachts. This paper addresses this issue and aims to answer the following research question by using machine learning methods:

\begin{quote}
\emph{RQ: How well does an applicant fit into a position at M Yachts?}
\end{quote}

\noindent The fit of an applicant shall be represented by a star rating between zero and five, where five is a perfect fit and zero represents the lowest matching. The star rating is currently determined manually by a recruiter, which poses a lot of manual work and challenges the company’s resources. The motivation of the paper is to learn the application process to reduce those costs on the one hand and provide an objective judgement to improve the quality of the yachting employees on the other hand. 
The article is written as a prediction challenge of the module Machine Learning at the University of Cologne, held by Prof. Dr. Markus Weinmann, in cooperation with the company M Yachts. Goals of the module are to apply the main concepts of machine learning and analyse results of them, competencies which are applied to a practical problem of M Yachts in this task. 

\noindent First, the paper deals with exploratory data analysis and describes the characteristics of the given data set. Second, different machine learning methods are presented to find the best prediction whether an applicant fits M Yachts or not. After introducing the most important findings, the paper will discuss both usability and validity of the results, as well as ethical applicability. In the end, the article gives an outlook to illuminate the extensibility of the data set and the potential of implementing other Machine Learning Tools to improve the application process.


\section{Dataset}

To ensure that the data used for training machine learning models is of high quality, accurate and properly formatted, we define variables of interest, apply data wrangling and perform an exploratory data analysis (EDA) on the prepared dataset. The dataset is provided by M Yachts and can be viewed in the included csv file ([MyCrew Data_221208.csv](./MyCrew Data_221208.csv)).

```{r, echo=FALSE, message=FALSE}
# Load raw dataset
mycrew <- read.csv("MyCrew Data_221208.csv", na.strings = c("", "NA","na", "n/a", "N/A"))

# Replace data that leads to Latex error
mycrew$City <- replace(mycrew$City, mycrew$City=="Ден-Хелдер","Den Helder")
```
```{r, echo=FALSE, message=FALSE, results=FALSE}
summary(mycrew)
```

\subsection{Variables of Interest}

To optimise the model's performance we filter variables that are not suitable as predictors. Therefore, we obtained an overview of the raw data by examining each variable and checking for correlations between the variables and the outcome \textit{StarRating}. This leads to the exclusion of 15 variables, including variables we do not consider to be causally related to the Star Rating (\textit{WorkerID, IsTempProfile, WorkerJobSearchStatus, ConnectionPipelineStatus, ProfileAddedAt, ProfileAddedBy, TalentPoolCount}), variables that contain mostly missing data (\textit{RecruiterMessage, WorkerMessage, TotalResponseRate, LastCommunicatedAt, LastCommunicatedBy}) and variables about the origin of the people that are too specific and will be represented by the variable \textit{Country} (\textit{State, City, Postcode}). This leaves us with the four character variables \textit{CurrentCompany}, \textit{CurrentRole}, \textit{Country} and \textit{TalentCommunitySource} and two integer variables \textit{ExperienceYears} and \textit{CvStrength} of our selected dataset (see Figure 1).

```{r, echo=FALSE, results=FALSE}
# Mutate StarRating into numeric
mycrew <- mycrew %>%
  mutate( StarRating = ifelse(StarRating == "OneStarReview",1,
                       ifelse(StarRating == "TwoStarReview",2,
                       ifelse(StarRating == "ThreeStarReview", 3,
                       ifelse(StarRating == "FourStarReview", 4,
                       ifelse(StarRating == "FiveStarReview", 5,
                       ifelse(StarRating == "NotSuitable", 0,"NA")))))),
          StarRating = as.numeric(StarRating))

# Checking for correlations and its significance
rcorr(as.matrix((mycrew[c("WorkerID","IsTempProfile","TalentPoolCount","WorkerMessage","TotalResponseRate","CvStrength","ExperienceYears","RecruiterMessage","StarRating")])))
```

```{r, echo=FALSE, results=FALSE}
# Number of NA's in each column:
sort(colSums(is.na(mycrew)))
```

\subsection{Data Wrangling}

Data Wrangling is referred to as "the process of gathering, selecting, and transforming data to answer an analytical question" \cite{elderresearch}, which in our case results in a prepared dataframe [df] used for various machine learning models (see Figure 1). Specifically, we transform our raw dataset by mutating relevant variables and by converting categorial variables to factors and numeric variables to numeric values. With data enrichment we aim to expand the scope and context of our analysis and potentially improve the accuracy of the prediction.

\begin{figure}
  \includegraphics{DataWranglingFig.pdf}
  \caption{Data Wrangling Process}
\end{figure}

The original variable \textit{TalentCommunitySource} is transformed so that its attributes are unified into "LinkedIn","Indeed","Google","my-crew.com" and "Other" and saved as \textit{TalentSource}. The variables \textit{CurrentRole} and \textit{CurrentCompany} are matched with job, respectively industry categories by specifying a list of keywords for each category and filtering the attributes that contain those keywords. Therefore, we use ifelse statements including the grepl function.

To get an overview of different categories in both columns, we first performed the unsupervised method of k-means clustering by transforming the words into numeric values and finding clusters within those words. By plotting the "Within groups sum of squares" we identified 11 means for the clustering (see Figure 2). This way we found clusters such as "Head Chef", "Manager" or "Director" for \textit{CurrentRole}. With those insights we classified the variable into the following groups:

\begin{table}[h!]
\raggedright
\begin{tabular}{|c|c c c c c c c c|} 
 \hline
 Jobs: & Allrounder & Captain & Deckhand & Engineer & Chef & Steward & Unemployed & Other \\ [0.5ex] 
 \hline
 n: & x & x & x & x & x & x & x & x \\ [1ex]
 \hline
\end{tabular}
\begin{tabular}{|c|c c c c c c c|} 
 \hline
 Industries: & airline & defence & freelance & gastro & hotel & yachting & Other \\ [0.5ex] 
 \hline
 n: & x & x & x & x & x & x & x \\ [1ex]
 \hline
\end{tabular}
\caption{Attributes in \textit{Jobs} and \textit{Industry}}
\label{table:1}
\end{table}

```{r, echo=FALSE, results=FALSE}
# Mutate TalentSource, convert and select variables 
df1 <- mycrew %>%
  mutate( TalentSource = 
                        ifelse(TalentCommunitySource == "LinkedIn", "LinkedIn",
                        ifelse(TalentCommunitySource == "Careers Website",
                               "Careers Website",
                        ifelse(TalentCommunitySource == "Indeed","Indeed",
                        ifelse((grepl("linked", TalentCommunitySource, 
                                      ignore.case = TRUE)) == TRUE, "LinkedIn",
                        ifelse((grepl("google", TalentCommunitySource, 
                                      ignore.case = TRUE)) == TRUE, "Google",
                        ifelse((grepl("crew", TalentCommunitySource,
                                     ignore.case = TRUE)) == TRUE, "my-crew.com", 
                        "Other")))))), # unify TalentCommunitySource variable
          TalentSource = as.factor(TalentSource),
          StarRating = as.numeric(StarRating),
          CurrentCompany = as.factor(CurrentCompany),
          CurrentRole = as.factor(CurrentRole),
          Country = as.factor(Country)) %>%
  select(CurrentCompany, CurrentRole, StarRating, Country, TalentSource, ExperienceYears, CvStrength)
```

```{r, echo=FALSE, results=FALSE}
# function for WSSPlot (k-means clustering)
# function from: https://rdrr.io/github/dgarmat/dgfunctionpack/man/wssplot.html
wssplot <- function(df, nc = 15, seed = 1234, nstart = 1){
  wss <- (nrow(df) - 1) * sum(apply(df, 2, var))
  for (i in 2:nc){
    set.seed(seed)
    wss[i] <- sum(kmeans(df, centers = i, nstart = nstart)$withinss)}
  qplot(x = 1:nc, y = wss,  xlab="Number of Clusters",
        ylab="Within groups sum of squares") + geom_line() +
    theme(axis.title.y = element_text(size = rel(.8), angle = 90)) +
    theme(axis.title.x = element_text(size = rel(.8), angle = 00)) +
    theme(axis.text.x = element_text(size = rel(.8))) +
    theme(axis.text.y = element_text(size = rel(.8)))
}
```

```{r, echo=FALSE, results=FALSE}
jobs <- as.vector(df1$CurrentRole)

dtm_j <- DocumentTermMatrix(Corpus(VectorSource(jobs)))
```

```{r, echo=FALSE, fig.align="center", fig.pos="h!", out.width="70%", fig.cap="WSS Plot for k-means Clustering"}
wssplot(dtm_j, nc=25,seed=1234,nstart=1)
```

```{r, echo=FALSE, results=FALSE}
km <- kmeans(as.matrix(dtm_j),centers=11, algorithm="Hartigan-Wong")

df_jobs <- data.frame(job = jobs, cluster = km$cluster)

#df_jobs %>%
 # group_by(cluster) %>%
 # summarise(count = n())
  
df_jobs$cluster[df_jobs$cluster == 1] <- "Head Chef"
df_jobs$cluster[df_jobs$cluster == 2] <- "Chef"
df_jobs$cluster[df_jobs$cluster == 3] <- "Director"
df_jobs$cluster[df_jobs$cluster == 4] <- "Instructor"
df_jobs$cluster[df_jobs$cluster == 5] <- "Other"
df_jobs$cluster[df_jobs$cluster == 6] <- "Executive Job"
df_jobs$cluster[df_jobs$cluster == 7] <- "Engineer"
df_jobs$cluster[df_jobs$cluster == 8] <- "Cabin Crew"
df_jobs$cluster[df_jobs$cluster == 9] <- "Deckhand/Master/Steward"
df_jobs$cluster[df_jobs$cluster == 10] <- "Manager" 
df_jobs$cluster[df_jobs$cluster == 11] <- "Assistant"
```

```{r, echo=FALSE, results=FALSE}
# Match CurrentRole with job categories:

# - Allrounder (if more than one job category is specified)
# - Captain
# - Deckhand
# - Engineer
# - Chef
# - Steward

# Add senior and leader to further specify "high quality" roles

chef <- c("chef", "cook", "cocin", "kitch","cozin", "culinar", "cuisin", "capoc", "sous")
engineer <- c("neer", "tech", "engin", "ingegn", "engen", "ingen")
deckhand <- c("deck", "bosun", "marin", "seaman", "mozzo", "shipmate", "cadet")
unemployed <- c("unempl", "seeking", "jobless", "search", "none")
steward <- c("stew", "cabin", "crew", "waiter", "waitress")
captain <- c("ptain", "skipper", "capit", "patron")
senior <- c("senior", "first", "1st", "director" , "leader", "head", "chief", "master", "jefa", "jefe")
leader <- c("senior", "first", "1st", "director" , "leader", "head", "chief", "master", "jefa", "jefe", "manager")


df2 <- df1 %>%
  mutate(Job = ifelse((grepl(captain, CurrentRole, ignore.case = TRUE)) == TRUE &
                      (grepl(paste(engineer, collapse = "|"), CurrentRole, ignore.case = 
                      TRUE)) == TRUE ,"Allrounder",
               ifelse((grepl(captain, CurrentRole, ignore.case = TRUE)) == TRUE &
                      (grepl(paste(steward, collapse = "|"), CurrentRole, ignore.case = TRUE)) == TRUE ,
                      "Allrounder",
               ifelse((grepl(captain, CurrentRole, ignore.case = TRUE)) == TRUE &
                      (grepl(paste(deckhand, collapse = "|"), CurrentRole, ignore.case = 
                      TRUE)) == TRUE ,"Allrounder",
               ifelse((grepl(captain, CurrentRole, ignore.case = TRUE)) == TRUE &
                      (grepl(paste(chef, collapse = "|"), CurrentRole, ignore.case = 
                      TRUE)) == TRUE ,"Allrounder", 
                      #---------------------------------------------------------
               ifelse((grepl(paste(engineer, collapse = "|"), CurrentRole, ignore.case = 
                      TRUE)) == TRUE & (grepl(paste(steward, collapse = "|"), CurrentRole, ignore.case = TRUE)) 
                      == TRUE , "Allrounder",
               ifelse((grepl(paste(engineer, collapse = "|"), CurrentRole, ignore.case = 
                      TRUE)) == TRUE & (grepl(paste(deckhand, collapse = "|"),  
                      CurrentRole, ignore.case = TRUE)) == TRUE ,"Allrounder",
               ifelse((grepl(paste(engineer, collapse = "|"), CurrentRole, ignore.case = 
                      TRUE)) == TRUE & (grepl(paste(chef, collapse = "|"), CurrentRole, 
                      ignore.case = TRUE)) == TRUE , "Allrounder", 
                      #--------------------------------------------------------
               ifelse((grepl(paste(steward, collapse = "|"), CurrentRole, ignore.case = TRUE)) == TRUE &
                      (grepl(paste(deckhand, collapse = "|"), CurrentRole, ignore.case = 
                      TRUE)) == TRUE , "Allrounder",
               ifelse((grepl("stew", CurrentRole, ignore.case = TRUE)) == TRUE &
                      (grepl(paste(chef, collapse = "|"), CurrentRole, ignore.case = 
                      TRUE)) == TRUE , "Allrounder", 
                      #--------------------------------------------------------
               ifelse((grepl(paste(deckhand, collapse = "|"), CurrentRole, ignore.case = 
                      TRUE)) == TRUE & (grepl(paste(chef, collapse = "|"), CurrentRole, 
                      ignore.case = TRUE)) == TRUE ,"Allrounder", 
                      
               # Next: assign jobs by finding key words in CurrentRole
               ifelse((grepl(captain, CurrentRole, ignore.case = TRUE)) == TRUE,"Captain",
               ifelse((grepl(paste(deckhand, collapse = "|"), CurrentRole, ignore.case = 
                     TRUE)) == TRUE,"Deckhand",
               ifelse((grepl(paste(steward, collapse = "|"), CurrentRole, ignore.case = TRUE)) == TRUE,"Steward",
               ifelse((grepl(paste(engineer, collapse = "|"), CurrentRole, ignore.case = 
                     TRUE)) == TRUE,"Engineer",
               ifelse((grepl(paste(chef, collapse ="|"), CurrentRole, ignore.case = 
                     TRUE)) == TRUE,"Chef",
               ifelse((grepl(paste(unemployed, collapse ="|"), CurrentRole, ignore.case = 
                     TRUE)) == TRUE, "Unemployed", "Other"))))))))))))))))) %>%
  mutate(HighPerformer = ifelse((grepl(paste(senior, collapse = "|"), CurrentRole, ignore.case = TRUE)) == TRUE, 1, 0),
         Leader = ifelse((grepl(paste(leader, collapse = "|"), CurrentRole, ignore.case = TRUE)) == TRUE, 1, 0)) %>%
  
  mutate(Job = as.factor(Job))
```

```{r, echo=FALSE, results=FALSE}
# Define industries:
airline <- c("line", "air", "wings", "ryanair", "easyjet", "jet", "fly", "flight")
yachting <- c("yach", "boat", "chart", "ship", "crew", "sail", "ocean", "farry",
              "coast", "cruise", "wave", "motor", "acht", "yac", "yate", "barco",
              "water", "nautic", "ferrie", "freight", "cargo", "royal caribbean",
              "10m", "20m", "30m", "40m", "50m", "46m", "55m", "100m", "107m", "M/Y",
              "catamaran")
hotel <- c("hotel", "hyatt", "hilton", "resort", "lux", "accom")
gastro <- c("restaur", "baker", "bar", "food", "ristorante", "caffe", "coffe",
            "steak", "cafe", "beef", "cake", "catering", "chef", "burger",
            "gastro", "Café", "cook", "breakfast", "mcdonald", "resto")
defence <- c("minist", "defenc", "marine", "navy", "force", "army", "marit",
             "safety", "security", "red cross", "milit")
freelance <- c("freelance", "self", "free lance")
# chains <- c("ryanair", "walmart", "carrefour", "easyjet")


df3 <- df2 %>%
  mutate(Industry = 
           ifelse((grepl(paste(yachting, collapse = "|"), CurrentCompany, ignore.case = 
           TRUE)) == TRUE,"yachting",
           ifelse((grepl(paste(gastro, collapse = "|"), CurrentCompany, ignore.case = 
           TRUE)) == TRUE,"gastro",
           ifelse((grepl(paste(hotel, collapse = "|"), CurrentCompany, ignore.case = 
           TRUE)) == TRUE,"hotel",
           ifelse((grepl(paste(defence, collapse = "|"), CurrentCompany, ignore.case = 
           TRUE)) == TRUE,"defence",
           ifelse((grepl(paste(freelance, collapse = "|"), CurrentCompany, ignore.case = 
           TRUE)) == TRUE,"freelance",
           ifelse((grepl(paste(airline, collapse = "|"), CurrentCompany, ignore.case = 
           TRUE)) == TRUE,"airline", "Other"))))))) %>%
  mutate(Industry = as.factor(Industry))
```

```{r, echo=FALSE, results=FALSE}
# Read in addional data frame
# Rename variables
# Convert to american numbers format
# Merge with data from Myachts
additional_data <- read.csv("additional_data.csv", na.strings = c("", "NA", "N/A", "n/a"), sep=";") %>%     
    rename(Region = Economic.Region,
         Pop = Population..2020.,
         Coastline = Coastline.WRI..km.,
         CoastRatio = Coast.Area.Ratio..WRI...m.km2.,
         Happiness = Happiness.Report) %>%
  
  mutate(Region = as.factor(Region),
         Country = as.factor(Country),
         Pop = as.numeric(stri_replace_all_regex(Pop,
                                  pattern=c("\\.", ","),
                                  replacement=c("", "."),
                                  vectorize=FALSE)),
         Happiness = as.numeric(stri_replace_all_regex(Happiness,
                                  pattern=c("\\.", ","),
                                  replacement=c("", "."),
                                  vectorize=FALSE)),
         Coastline = as.numeric(stri_replace_all_regex(Coastline,
                                  pattern=c("\\.", ","),
                                  replacement=c("", "."),
                                  vectorize=FALSE)),
         CoastRatio = as.numeric(stri_replace_all_regex(CoastRatio,
                                  pattern=c("\\.", ","),
                                  replacement=c("", "."),
                                  vectorize=FALSE))) %>%
  select(Country, Region, Pop, Happiness, Coastline, CoastRatio)

# summary(additional_data)

# Merge data
df4 <- inner_join(df3, additional_data, by = "Country")
```

```{r, echo=FALSE, results=FALSE}
# Select variables of interest and remove NA's

# Selection
df5 <- df4# %>%
  #select(StarRating, CurrentCompany, CurrentRole, Country, Region,
         #Happiness, Coastline, CoastRatio, TalentSource, 
         #Pop, ExperienceYears, CvStrength, Job, Industry)
# Remove NAs
df6 <- na.omit(df5)
```

```{r, echo=FALSE, results=FALSE}
#summary(df5$Job) #including two new variables provided by MYachts (esp. Experience) reduces n by ~500
```

```{r}
# Add new variables such as tourism (obtained from WDI)
# Merge with previous dataset
Tourism_df <- read.csv("tourism_data_edited.csv", na.strings = c("..", ""), sep=",") %>%
  rename(Country = Country.Name,
         Year = Time,
         GDPpC = GDP.per.capita..PPP..constant.2017.international.....NY.GDP.PCAP.PP.KD.,
         Tourists = International.tourism..number.of.arrivals..ST.INT.ARVL.,
         Price_level = Price.level.ratio.of.PPP.conversion.factor..GDP..to.market.exchange.rate..PA.NUS.PPPC.RF.) %>%
  mutate(GDPpC = gsub(",",".",GDPpC),
         Tourists = gsub(",",".",Tourists),
         Price_level = gsub(",",".",Price_level),
         Country = as.factor(Country),
         GDPpC = as.numeric(GDPpC),
         Tourists = as.numeric(Tourists),
         Price_level = as.numeric(Price_level)) %>%
  select(Country, Year, GDPpC, Tourists, Price_level)

# Build three year averages of new predictors to ensure more reliable values
Tourism_avg <- Tourism_df %>%
  group_by(Country) %>%
  summarise(meanGDPpC = mean(GDPpC, na.rm = TRUE),
            meanPriceLevel  = mean(Price_level, na.rm = TRUE),
            meanTourists  = mean(Tourists, na.rm = TRUE)) %>%
  select(Country, meanGDPpC, meanPriceLevel, meanTourists)

# Merge Tourism data with previous dataset
df7 <- inner_join(df6, Tourism_avg, by = "Country")
setdiff(df5$Country, Tourism_avg$Country)

        
```

```{r, echo=FALSE, results=FALSE}
# Reduce number of countries (if n(Country) < 10 --> "Other")
frequent_countries <- df6 %>%
  group_by(Country) %>%
  count()

df8 <- left_join(df7, frequent_countries, by = "Country")

df9 <- df8 %>%
  mutate(Country_edited = ifelse(n < 10, "Other", as.character(Country)),
         Country_edited = as.factor(Country_edited),
         Tourists_pop = meanTourists / Pop,
         Tourists_coast = meanTourists / Coastline) %>%
  select(Job, StarRating, Industry, Country_edited,
         Region, Happiness, Coastline, CoastRatio, TalentSource, Pop, 
         ExperienceYears, CvStrength, HighPerformer, Leader,
         meanGDPpC, meanTourists, Tourists_coast, Tourists_pop, meanPriceLevel)

#Remove NAs
df10 <- na.omit(df9)
  
```

\subsection{Exploratory Data Analysis (EDA)}

EDA involves analyzing and summarizing the characteristics of a dataset, identifying patterns and anomalies, and visualizing the data to gain insights. This helps to understand the given data and identify any potential issues that may affect the accuracy of the used models. 

-> end with an outlook: In summary, the dataset does not appear perfectly suited for the prediction task. First of all, there is a There are only few continuous variables that can be directly attributed to the 

#Exploratory data analysis
```{r, echo=FALSE}
basic_eda <- function(data)
{
  glimpse(data)
  print(status(data))
  freq(data) 
  print(profiling_num(data))
  plot_num(data)
  describe(data)
}

basic_eda(df10)
```


#Check correlations
```{r, echo=FALSE}
library(corrplot)
df11 <- df10%>%
  select(StarRating, meanGDPpC, Happiness, ExperienceYears, CvStrength, Pop, 
         HighPerformer, Leader, Coastline, CoastRatio, meanTourists, meanPriceLevel,
         Tourists_pop, Tourists_coast)
  
  ggcorr(df11, label = TRUE)
  
```

#Highest rating: Engineer and Captain
#Lowest rating: Unemployed
--------------------------------------
#Highest experience: Captain, Engineer
#Lowest experience: Deckhand
--------------------------------------
#Highest CV score: Engineer
#Lowest CV score: Steward
--------------------------------------
#Rating and experience are moderately correlated (0.45)
```{r, echo=FALSE}
#StarRating per job category
#Highest rating: Engineer and Captain
#Lowest rating: Unemployed
n_job <- df10 %>%
  count(Job)

descriptive_rating_per_job <- df10 %>%
  group_by(Job) %>%
  summarise(meanRating = mean(StarRating),
            minRating = min(StarRating),
            maxRating = max(StarRating),
            sdRating = sd(StarRating)) %>%
  arrange(desc(meanRating)) %>%
  select(Job, meanRating, minRating, maxRating, sdRating)

(rating_per_job <- left_join(descriptive_rating_per_job, n_job, by= "Job"))

#Visualize
ggplot(rating_per_job, aes(Job, meanRating))+
  geom_bar(stat = "identity")

ggplot(df10, aes(Job, StarRating))+
  geom_boxplot()

#Experience per job category
#Highest experience: Captain, Engineer
#Lowest experience: Deckhand
descriptive_exp_per_job <- df10 %>%
  group_by(Job) %>%
  summarise(meanExp = mean(ExperienceYears),
            minExp = min(ExperienceYears),
            maxExp = max(ExperienceYears),
            sdExp = sd(ExperienceYears)) %>%
  arrange(desc(meanExp)) %>%
  select(Job, meanExp, minExp, maxExp, sdExp)
(exp_per_job <- left_join(descriptive_exp_per_job, n_job, by="Job"))

#CvStrength per job category
#Highest: Engineer
#Lowest: Steward
descriptive_cv_per_job <- df10 %>%
  group_by(Job) %>%
  summarise(meanCvStrength = mean(CvStrength),
            minCvStrength = min(CvStrength),
            maxCvStrength = max(CvStrength),
            sdCvStrength = sd(CvStrength)) %>%
  arrange(desc(meanCvStrength)) %>%
  select(Job, meanCvStrength, minCvStrength, maxCvStrength, sdCvStrength)
(cv_per_job <- left_join(descriptive_cv_per_job, n_job, by="Job"))

#Find correlations for jobs
join1 <- inner_join(rating_per_job, descriptive_exp_per_job, by = "Job")
(total_job_ratings <- inner_join(join1, descriptive_cv_per_job, by = "Job"))
#Rating and experience are moderately correlated (0.45)
cor(total_job_ratings$meanRating,total_job_ratings$meanExp)
cor(total_job_ratings$meanRating, total_job_ratings$meanCvStrength)
cor(total_job_ratings$meanExp, total_job_ratings$meanCvStrength)


```
#Rating of high performer lower than no high performer
#same for leader
```{r, echo=FALSE}
#Mean rating of high performer
descriptive_high_performer <- df10 %>%
  group_by(HighPerformer) %>%
  summarise(meanRating = mean(StarRating)) %>%
  arrange(desc(meanRating)) %>%
  select(HighPerformer, meanRating)

#Count number of HighPerformer
n_hp <- df10 %>%
  count(HighPerformer)
(rating_high_performer <- inner_join(descriptive_high_performer, n_hp, by = "HighPerformer"))


#Mean rating of leader
descriptive_leader <- df10 %>%
  group_by(Leader) %>%
  summarise(meanRating = mean(StarRating)) %>%
  arrange(desc(meanRating)) %>%
  select(Leader, meanRating)

#Count number of leader
n_leader <- df10 %>%
  count(Leader)
(rating_leader <- inner_join(descriptive_leader, n_leader, by = "Leader"))

```

#Same number of observations in each class of StarRating
```{r}
#Distribution of rankings
distr_of_ratings <- df10 %>%
  mutate(StarRating = as.factor(StarRating))
summary(distr_of_ratings$StarRating)
```
#Short: Candidates from yachting industry have superior rating, exp and cv score
#Highest rating in         yachting industry
#Highest CvStrength in     yachting industry
#Highest experience have   freelancers (2. defence, 3. yachting)
```{r}
#Count number of industry
n_indusry <- df10 %>%
  count(Industry)

#Rating per industry
descriptive_industry_rating <- df10 %>%
  group_by(Industry) %>%
  summarise(meanRating = mean(StarRating)) %>%
  arrange(desc(meanRating)) %>%
  select(Industry, meanRating)
(rating_industry <- inner_join(descriptive_industry_rating, n_indusry, by = "Industry"))

#Experience per industry
descriptive_industry_exp <- df10 %>%
  group_by(Industry) %>%
  summarise(meanExp = mean(ExperienceYears)) %>%
  arrange(desc(meanExp)) %>%
  select(Industry, meanExp)
(exp_industry <- inner_join(descriptive_industry_exp, n_indusry, by = "Industry"))

#CvStrength per industry
descriptive_industry_cv <- df10 %>%
  group_by(Industry) %>%
  summarise(meanCV = mean(CvStrength)) %>%
  arrange(desc(meanCV)) %>%
  select(Industry, meanCV)
(rating_industry <- inner_join(descriptive_industry_cv, n_indusry, by = "Industry"))
```

#Short: Experience in yachting seems to increase the rating over all jobs 
#       except for engineers
#Best rating for chefs with experience in yachting
#Best rating for captains with experience in freelance, defence, yachting
#Best rating for deckhand with experience in yachting (place 1 ignored since n=1)
#Best rating for steward with experience in airline, yachting
#Best rating for chefs with experience in yachting (place 1 ignored since n=1)
#Best rating for engineers with experience in "Other"
#Best rating for others with experience in hotel and yachting
```{r}
#Are experience in yachting industry beneficial for every job?
job_and_industry <- df10 %>%
  group_by(Job, Industry) %>%
  summarise(meanRating = mean(StarRating)) %>%
  arrange(desc(meanRating)) %>%
  select(Job, Industry, meanRating)

n_job_and_industry <- df10 %>%
  group_by(Job) %>%
  count(Industry)

rating_job_industry <- inner_join(job_and_industry, n_job_and_industry, by = c("Job", "Industry"))

#Chefs with yachting experience have better ratings
rating_job_industry %>%
  filter(Job == "Chef")

#Engineer's rating does not depend on industry expertise
rating_job_industry %>%
  filter(Job == "Engineer")

#Most captains have yachting experience as expected (many others couldn't be filtered)
rating_job_industry %>%
  filter(Job == "Captain")

#Yachting experience beneficial for rating
rating_job_industry %>%
  filter(Job == "Deckhand")

#Same as Deckhand, but best rating with airline experience
rating_job_industry %>%
  filter(Job == "Steward")

#Yachting industry again beneficial
rating_job_industry %>%
  filter(Job == "Allrounder")

#For unknown job categories, the best ratings are found in hotel and yachting
rating_job_industry %>%
  filter(Job == "Other")

```

#Best avg. rating in: Saudi Arabia, South Africa, Phillipines
#Results unstable since n is dominated by Spain with 273 obs. <---------
#e.g.: Best captains come from Germany

```{r}
#Rating per country
descriptive_country_rating <- df10 %>%
  group_by(Country_edited) %>%
  summarise(meanRating = mean(StarRating),
            minRating = min(StarRating),
            maxRating = max(StarRating),
            sdRating = sd(StarRating)) %>%
  arrange(desc(meanRating)) %>%
  select(Country_edited, meanRating, minRating, maxRating, sdRating)

n_country <- df10 %>%
  count(Country_edited)

(rating_countries <- inner_join(descriptive_country_rating, n_country, by= "Country_edited"))

#Job & Country
job_and_country <- df10 %>%
  group_by(Job, Country_edited) %>%
  summarise(meanRating = mean(StarRating)) %>%
  arrange(desc(meanRating)) %>%
  select(Job, Country_edited, meanRating)

n_job_and_country <- df10 %>%
  group_by(Job) %>%
  count(Country_edited)

rating_job_country <- inner_join(job_and_country, n_job_and_country, by = c("Job", "Country_edited"))
rating_job_country <- rating_job_country %>%
  filter(n > 2)
```

\section{Methods}

To really understand what machine learning methods are used and how they work, it is important to first understand what machine learning is: 
Machine learning is a subfield of artificial intelligence that involves the development of algorithms and models that can learn from and make predictions or decisions based on data. These algorithms and models are able to improve their performance over time as they are exposed to more data.

In machine learning, algorithms are trained on a dataset, which consists of a collection of examples that include input data and the corresponding correct output. The algorithm uses this training data to learn how to map the input data to the correct output. Once the algorithm has learned from the training data, it can then be applied to new, unseen data to make predictions or decisions.

There are several different types of machine learning, including supervised learning and unsupervised learning. Supervised learning involves training an algorithm on labeled data, where the correct output is provided for each example in the training data. Unsupervised learning comprises training an algorithm on unlabeled data, where the algorithm must discover the underlying structure of the data on its own.

\subsection{Supervised Learning}

In supervised learning, the training data consists of a set of input-output pairs. The algorithm uses these pairs to learn the relationship between the input and the output. Once the algorithm has learned this relationship, it can then be applied to new, unseen input data to predict the corresponding output.

Examples of supervised learning tasks include classification, where the goal is to predict a class label for a given input data, and regression, where the objective is to predict a continuous output value.

Supervised learning is widely used in many applications, such as image classification, spam detection, and stock price prediction. It is a powerful tool for solving problems that require the prediction of a specific output value based on input data.

\subsubsection{Classification}

In classification, the input data is usually represented as a set of features, which are attributes or characteristics of the data.
The class label is the output that the algorithm is trying to predict. For example, in a binary classification problem, there are only two possible class labels (e.g., spam or not spam). In a multi-class classification problem, there are more than two possible class labels.

The goal of the paper is to predict the star rating of the given data set which can be interpreted as a multi-class classification with score from zero to five. Thus, different classification methods could be a good approach to find an answer to the research question. 
While the research problem could also be interpreted as a regression problem, classification algorithms seem more intuitive and suitable to solve it, which is why a major part of the model selection will deal with classification and only a minor part will deal with regression methods.
In the following section we are going to discuss the suitability of common classification methods and how to code them in R.

\subsubsection{Decision Tree}

A decision works by creating a tree-like model of decisions and their possible consequences. The tree is created by splitting the data into smaller and smaller subsets based on certain feature values. At each node in the tree, the model makes a decision based on the value of a certain feature and directs the data to the right branch of the tree. The final decision at the end of the tree is the predicted outcome.

Decision trees are easy to understand and interpret, and they can handle both continuous and categorical data. However, they can be prone to overfitting and may not always perform as well as other machine learning algorithms.

The tree depth is the main parameter in a decision tree. It can be determined by setting a minimum number of data points for a node to get split further, a maximum tree depth, or can be driven by a cost-complexity parameter (used in this application). The cost-complexity parameter is another way to prune the tree, so that it controls the trade-off between the tree’s complexity and accuracy, in other words the trade-off between bias and variance. It is chosen by cross validation to find the value that results in the best performance on the test data.

!Cost complexity Parameter!

\subsubsection{Extreme Gradient Boosting XGBoosting}

!Tabelle mit Tuning Parametern die gewählt wurden!

\subsubsection{Random Forest}

The random forest algorithm is one of the most famous and widely used machine learning algorithms nowadays. It is an ensemble method, which means that the algorithm combines the predictions of multiple individual models to make a final prediction. In a random forest, these individual models are decision trees.

To construct a random forest, many decision trees are trained on bootstrapped samples of the training data and a random subset of the features. The final prediction is made by averaging the predictions of all the individual decision trees. This averaging process helps to reduce the variance and improve the overall performance of the model. 

As the random forest classifier can handle both continuous and categorical features and is able to predict multiple classes as an output, it is perfectly suited for the decision problem. To run the random forest algorithm there are a few hyperparameters that can be chosen upfront or tuned. The main hyperparameters, when running a random forest algorithm in R are the number of trees the algorithm includes, the number of predictors randomly sampled at each split and the minimum number of data points in a tree node required to split the node further. 

In this application the hyperparameters are tuned using 10-fold cross validation. 
This means the data is split into 10 folds, and the model is trained and evaluated on different subsets of the training data. This allows for a more robust evaluation of the model’s performance, as it is tested on a variety of different data points.

\subsubsection{Naive Bayes}

Naive Bayes is a simple and effective probabilistic classifier based on applying Bayes' theorem with strong independence assumptions (naive) between the individual predictors. As it can be used as a multiclass classifier and is able to handle continuous and categorical data as features 

In the context of machine learning, naive Bayes can be used to classify data points based on their features. The algorithm makes the assumption that the features are independent of one another, which is known as the "naive" assumption. This assumption simplifies the calculations and allows the algorithm to be implemented efficiently.
To classify a new data point, the algorithm calculates the probability of the data point belonging to each class. The class with the highest probability is chosen as the prediction for the data point. The probability is calculated based on the prior probability of the class and the likelihood of the features given the class.

\subsubsection{Other Classification Algorithms}

Another algorithm that can be used for classification is a neural network, often described with the term “Deep Learning”. A neural network is a type of machine learning model that is inspired by the structure and function of the brain. It consists of a large number of interconnected processing units (neurons) that are organised into layers, and it is trained using a large dataset and an optimization algorithm. Neural networks are able to learn complex relationships between input and output data. 

While a neural network is generally well suited for multi-class classification, the performance of a neural network will depend on the size and quality of the training dataset. Since our dataset is rather small and of medium to low quality, the neural network may not be able to learn the necessary relationships and may not perform well. Furthermore, the interpretability of a neural network is very small, since it uses a “black-box approach”, so results of this model might not be as helpful for us and MYachts as results of other classifiers like decision trees. Since a Neural Network is also hard to tune and optimise, we decided not to use it on the dataset.

Another classification algorithm is the Support Vector Machine (SVM). In its base form, SVM’s divide a dataset into two classes using a hyperplane that maximises the separation of the data points to their potential classes in an n-dimensional space. The data points with the smallest distance to the hyperplane are called Support Vectors. 
We decided against this algorithm because it is difficult to apply to multiclass classification problems - The usual way to achieve this is to divide the multiclass problem into multiple binary classifications, which is beyond the scope of this paper.

\subsubsection{Regression}

While classification algorithms are used to predict a discrete class label for a given input, regression algorithms predict a continuous output value. 
So far we looked at classification algorithms, because the star rating is in itself discrete - but the way a star rating is given leaves room for the possible use of regression algorithms:
For example, not every 2-star review is as good as another. A reviewer might see a profile as a high 2-star or a low 2-star profile. Therefore it is possible to interpret our outcome variable as continuous, just with a rounded result to derive at the next closest full number. 

Both classification algorithms and regression algorithms might lead to valid results in this case and it is therefore sensible to also explore regression methods.

When performing regression methods in this context, we could measure them on mean standard error (MSE), which is a measure of difference between the prediction and the actual star rating. One advantage of this approach is that it penalises very wrong predictions (e.g. a 1 star prediction on a 5 star candidate) more heavily. On the other hand we either have to round our predictions to the next full number, or have to accept that there’s always gonna be differences between prediction and truth, because only the prediction allows for decimal values. Furthermore, a regression model might also predict values below 0 or above 5 for extreme cases, which is not a possibility in a real-life context. 
To compare the performance better to the performance of classification methods, we can also choose to assess model success using accuracy, by rounding our results to the nearest full number and checking whether or not that is the correct rating. 

The most basic kind of regression is linear regression: it is a method used to model the linear relationship between a dependent variable and one or more independent variables. Generally it is used to model relationships between continuous variables, rather than categorical variables, which poses a problem for our predominantly categorical dataset. 
The method can be used with categorical variables, but the variables need to be transformed into numerical form in order for the model to be able to work with them. One way to do this is to create dummy variables for each category. A dummy variable is a binary variable that takes on the value of 1 if the observation belongs to a particular category, and 0 otherwise. Of course this is challenging for variables like “Country”, which has more than 100 factors, requiring us to create more than 100 dummy variables for just this one predictor. 
If we want to use regression methods, it might make sense to drop the factors with most levels entirely, since their singular dummy variables provide very little explanative value. 

Ridge regression and lasso regression are two techniques that can be used to improve the performance of linear regression models. These techniques are known as regularisation methods, because they add a penalty term to the model's objective function to help reduce overfitting and improve the model's generalisation to new data.

\paragraph{Linear Regression}

To summarise, regression methods generally are not perfectly suited for our dataset due to the nature of the many multi-level categorical variables, however they might yield a benefit in the interpretation by allowing decimal values.

\subsection{Unsupervised Learning}

Unsupervised learning is a type of machine learning in which the model is not given any labeled training data. Instead, the model must find patterns and relationships in the data on its own. This is in contrast to supervised learning, in which the model is given labeled training data and learns to make predictions based on that data.

Unsupervised learning is useful for exploring and understanding the structure of a dataset, and it can be used to generate new features or to identify patterns that may not be apparent in the raw data. However, it is generally less accurate than supervised learning and requires more data to be effective.

The machine learning field can be divided into dimensionality reduction and clustering.

\subsubsection{Dimensionality Reduction}
Principal Component Analysis:

\subsubsection{Clustering}
\paragraph{K-Means}

\section{Results}
Model evaluation and results of the different models
Which algorithm/ML method has the best accuracy? 

Why do we choose it and what could we change? 


\subsection{Plots}

\section{Discussion}
What challenges did we face in the prediction challenge? 
Data lack potential
\subsection{Challenges}
\subsection{Ethics}
Ethics (country and crime rate as predictor) 
\subsection{Outlook}
How to improve the application process? 
\newpage
\section{Evaluation of examination results}

\begin{enumerate}

\item \textbf{Same grade: We confirm that each student from our group has contributed equally to the project.}


  \begin{tabular}{@{}p{7cm}p{5cm}@{}}
  \\
  \hrulefill \\
  (Name, matriculation No.) \\
  \\
  \hrulefill \\
  (Name, matriculation No.) \\
  \\
  \hrulefill \\
  (Name, matriculation No.) \\
  \\
  \hrulefill \\
  (Name, matriculation No.) \\
  \end{tabular}
\\


\item \textbf{Individual grades: We request individual grades for each student from our group.}

Please describe \textbf{in detail} the contributions of each group member in the following:


\textit{Student 1:}
    
  \begin{tabular}{@{}p{7cm}p{5cm}@{}}
     \\
     \hrulefill \\
      (Signature)
   \end{tabular}
 \newline 
 
\textit{Student 2:}
  
    \begin{tabular}{@{}p{7cm}p{5cm}@{}}
     \\
     \hrulefill \\
      (Signature)
   \end{tabular}
 \newline 
 
\textit{Student 3:}

    \begin{tabular}{@{}p{7cm}p{5cm}@{}}
     \\
     \hrulefill \\
      (Signature)
   \end{tabular}
\newline

\textit{Student 4:}
  
    \begin{tabular}{@{}p{7cm}p{5cm}@{}}
     \\
     \hrulefill \\
      (Signature)
   \end{tabular}


\end{enumerate}

\newpage
\section{Bibliography}

\printbibliography

\setlength{\parindent}{0em}



